% !TeX root = ../main.tex

\chapter{A Fully-Convolutional LSTM-Autoencoder for Frame Prediction} \label{chapter:implementation}

- Present our model/implementation/architecture

- References on which other models/ideas everything is based

\section{2D convolutional LSTM} \label{sec:conv_lstm}

Explain it mathematically (compared to classical LSTM).
Also explain the introduced Batch-Normalization (based on the other paper).
We used with peepholes.

To our own knowledge, our framework has the first TensorFlow implemention of it.


\section{Architecture/Model}

% because we use an "autoencoder/enc-dec-style" model: argue why this model does not learn the trivial function: see "Unsupervised LSTM paper" (yellow): 1. fixed repr. size but unlimitit long sequence 2. recursive representation: it is applied at each time-step of the decoder, same dyn. must be applied each time


\subsection{Image Percepts}

Why we use image percepts and not the raw image.
(Memory?, Makes the model deeper, ...)

\subsection{Schedules Sampling}

Explain schedules sampling and the problem why it is needed. Compare it agains "Always sampling".


\section{Training procedure}

OR IS THIS A SEPARETE CHAPTER (Model Evaluation) and our Evaluation Chapter is "Analysis"

\subsection{Preprocessing}

\subsection{Loss function}

- Squared error, cross-entroy, MS-SSIM?

- Compare these, what are its pros and cons

- Why is  selecting the loss function so important?

\subsection{Hyperparameters}

\section{TensorFlow}

We implemented our model in TensorFlow v0.10, CUDA 7.5.18 and cuDDN 4.





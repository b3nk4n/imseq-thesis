% !TeX root = ../main.tex

% for every dataset-group: tell which loss_main was used by default (if not stated differently)
% tell we use batch_size of 32 always (without any reason...)

% performed on: We implemented our model in TensorFlow v0.10, CUDA 7.5.18 and cuDDN 4.

% explore the model on MovingMNIST (layers, scheduled samling, filter depth)
%      --> fix the main hyper-params
% small grid search on each model?
%      --> compare results

%Wie gehen wir beim Training vor: Wegen sehr vielen Hyperparmeter: Nutzen MM um die einzelnen HParams unanbh√§ngig voneinander zu erforschen. Dann: Grid-Search auf den jeweils besten Einstellungen

\chapter{Evaluation} \label{chapter:evaluation}

This chapter presents the evaluation results of our model on all datasets of chapter \ref{chapter:datasets}. Since the model exhibits a vast number of hyper parameters and each training can take several days, performing an extensive grid search is not possible. That's why we start to explore the model's substantial hyperparameters independently from each other by using the simplest dataset. Afterwards, the acquired knowledge about our model's behavior can then be transfered to the other datasets in order to to a basic grid search. Beside the quantitative assessment of the model, it is also qualitatively compared to experimental results of related projects at the end of each section.

All experiments have been computed on a single Nvidia GeForce GTX Titan X using the TensorFlow open source software library for machine intelligence with version r0.10. This quite new deep learning framework has been developed by Google as a second-generation system based on their experiences on DistBelief \parencite{distbelief}. A computation in TensorFlow is expressed as a statefule dataflow graph, where each node represents a mathematical operation and the graph's edges a data tensor that can flow from one node to the next \parencite{tensorflow2015-whitepaper}. It has gained increasing attention since its first public release in November 2015.

If not stated differently, the model configuration described in Chapter \ref{chapter:implementation} is used, but with a two-layer ConvLSTM using $3\times3$ input-to-hidden and $5\times5$ hidden-to-hidden kernel size in the internal convolutions. Further, we have deactivated the optional BN-layers within these cells for two reasons. Firstly, first tests have not shown any improvements using them. And secondly, the fact that its TensorFlow implementation back then had the issue\footnote{Further details: \url{https://github.com/tensorflow/tensorflow/issues/4361}} of performing batch statistic updates multiple times when being share across time, resulted in an enormous drop in performance. We have therefore kept the use of these batch normalizations of the spatio-temporal encoder and decoder components for future work. Additionally, all weights are initialized using the Xavier method of equation \ref{eq:xavier} and the default weight decay regularization coefficient is set to $\lambda=1e^{-5}$. We use a batch size of 32 during the training process together with Adam optimizer ($\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon =1e^{-8}$) and a default initial learning rate of $\eta = 0.001$. In addition to the learning rate annealing mechanisim of Adam, the learning rate is also manually decreased once per epoch using a stair-case exponential decay function by a factor of $\alpha=0.9$.

Lastly, it has to be menioned that diagrams, which show results of any kind of objective function, are always shown with its y-axis in logarithmic scale so that fine differences become better recognizable. And unless otherwise specified, all losses represent the per-pixel error and averaged across the whole generated future sequence.

\section{Experiments on Moving MNIST} \label{sec:exp-mm}

The synthetic Moving MNIST dataset is used as a starting point for the evaluation of our model. Given an input sequence of ten frames, the network therefore has to understand and encode the motion of this input in order to predict the next ten frames of the future. In this section, we qualitatively and quantitatively compare our results with other network models, because it has been used in several previous works as well. The loss layer used in this section does not use the SSIM term by default.

\subsection{Model Exploration}

The model is incrementally explored in this section in order to understand its behaviour regarding changes in the hyperparameters, as well as to see the effects of the modern techniques used in the training process.

\subsubsection*{Scheduled Sampling}

The consequences of the used scheduled sampling technique is examined first. Figure \ref{fig:plot-mm-asss} visualizes the training and validation binary cross-entropy using our standard model with either scheduled sampling (SS) or always sampling (AS). The latter method means the approach to always sample from the previously generated frame, which is comparable to SS with a constant sampling probability of $p=0$. It can be seen that there is a huge discrepancy between the training and validation error in the starting phase, where the SS-approach mainly trained on input samples taken from the ground truth. The can be explained that sampling on the ground truth in every timestep of the recurrent network tremendously speeds up the convergence of the training loss, because each cell does not have to correct errors of the previous cells. In contrary, the validation loss is very bad in this phase, since the results represent the average out of ten predicted frames and the spatio-temporal decoder suddenly receives its own feature space predictions as input to predict the next frame, which is in contrast to the process while training so far.

\begin{figure}[htb]
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/as_ss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-YT-tag_validation_bce.csv}\modelA
  \pgfplotstableread{data/mm/as_ss/run_mm-as-2l3i5hp-c326464k533s212bn-wd1e-05-SE-tag_validation_bce.csv}\modelB
  \pgfplotstableread{data/mm/as_ss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-YT-tag_loss-BCE_loss-Mean.csv}\modelAA
  \pgfplotstableread{data/mm/as_ss/run_mm-as-2l3i5hp-c326464k533s212bn-wd1e-05-SE-tag_loss-BCE_loss-Mean.csv}\modelBB
  \pgfplotstableread{data/mm/as_ss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-YT-tag_scheduled_sampling.csv}\modelSS
  \begin{tikzpicture}
    \begin{axis}[
    	axis y line*=right,
    	hide x axis,
    	ylabel near ticks,
        ymin=0,
        ymax=1,
        xmin=0,
        xmax=65000,
        thick,
        ylabel=GT sampling probability \textit{p},
        xlabel=step \textit{i},
        x post scale=1.8,
      ]
      \addplot[draw=LightGoldenrod, thin, opacity=0.8] table[x=Step, y=Value]{\modelSS};\label{mSS}
    \end{axis}
    \begin{axis}[
    	axis y line*=left,
    	ymode=log,
    	log ticks with fixed point,
        ymin=0,
        ymax=0.3,
        xmin=0,
        xmax=65000,
        ytick={0.05,0.075,0.1,0.15,0.2},
        legend style={legend pos=north east},
        grid,
        thick,
        ylabel=BCE loss,
        xlabel=step \textit{i},
        x post scale=1.8,
      ]
	  \addlegendimage{/pgfplots/refstyle=mSS}\addlegendentry{Scheduled Sampling}    
      
      % background
      \addplot[draw=black!10!blue, thin, opacity=0.8] table[x=Step, y=Value]{\modelBB};
      \addlegendentry{AS (train)}
      \addplot[draw=black!10!green, thin, opacity=0.8] table[x=Step, y=Value]{\modelAA};
      \addlegendentry{SS (train)}
      %foreground
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{AS (valid)}
      %best
      \addplot[draw=black!30!green] table[dashed, x=Step, y=Value]{\modelA};
      \addlegendentry{SS (valid)} 
    \end{axis}
    
  \end{tikzpicture}
  \caption[Influences of Scheduled Sampling]{Influences of scheduled sampling regarding the training and validation error in context of recurrent networks and future frames prediction on Moving MNIST.}\label{fig:plot-mm-asss}
\end{figure}

But the interesting insight here is that as soon as the scheduled sampling component completely changed the input behaviour to inference mode, the achieved prediction performance is continiously better compared to the approach of always sampling from previous generatated frames directly at the very beginning. This behavior could be explained that the SS approach introduces some kind of pre-training phase, where the network can learn to predict the next frame when a perfect current frame is given. Hence, it has not to deal with errors made in the previous time step. By slowly changing this behaviour to the mode as it is used while inference, it then also starts to learn a robustness against imperfect input frames. A similar behavior can be seen when comparing the results of the PSNR metric between AA and SS in Figure \ref{fig:plot-mm-asss2-psnr}. However, when we look at the sharpeness difference metric in Figure \ref{fig:plot-mm-asss2-sdiffcurve}, it can be seen that sharpness of the predictions is continuously better using scheduled sampling.

\begin{figure}[htb]
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/as_ss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-YT-tag_validation_sharpdiff.csv}\modelA
  \pgfplotstableread{data/mm/as_ss/run_mm-as-2l3i5hp-c326464k533s212bn-wd1e-05-SE-tag_validation_sharpdiff.csv}\modelB
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymin=10,
        ymax=15,
        xmin=0,
        xmax=65000,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=SharpDiff,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{AS (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{SS (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-asss2-sdiffcurve}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/as_ss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-YT-tag_validation_psnr.csv}\modelA
  \pgfplotstableread{data/mm/as_ss/run_mm-as-2l3i5hp-c326464k533s212bn-wd1e-05-SE-tag_validation_psnr.csv}\modelB
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymin=10,
        ymax=20,
        xmin=0,
        xmax=65000,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=PSNR,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{AS (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{SS (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-asss2-psnr}
\end{subfigure}
\caption[Image Metrics Comparing AS and SS]{Comparison of validation results based on a model with either using the scheduled sampling or the always sampling training technique.} \label{fig:plot-mm-asss2}
\end{figure}



\subsubsection*{Batch Normalization}

The use of batch normalization in the spatial encoder and decoder components is investigated next. Figure \ref{fig:plot-mm-bn} shows a clear advantage of using BN-layers. It can be argued that our model is able to profit strongly from  batch normalization, because it does not only support the convolutional encoder and decoder to learn faster by compensating the internal covariate shift. Also, both recurrent networks in-between can benefit from this, reasoned by the fact that the frame representations in feature space, that are produced by the spatial encoders, tend to have a more stable distribution. Consequently, the spatio-temporal encoder has fewer problems to extract useful patterns from these representations while consuming them in the course of the training process.

\begin{figure}[htb]
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/bn/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-XG-tag_validation_bce.csv}\modelA
  \pgfplotstableread{data/mm/bn/run_mm-ss-2l3i5hp-c326464k533s212-wd1e-05-MX-tag_validation_bce.csv}\modelB
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymode=log,
    	log ticks with fixed point,
        ymin=0,
        ymax=0.3,
        xmin=0,
        xmax=65000,
        legend style={legend pos=north east},
        grid,
        thick,
        ylabel=BCE loss,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{w/o BN (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{with BN (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-bn-bce}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/bn/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-XG-tag_validation_sharpdiff.csv}\modelA
  \pgfplotstableread{data/mm/bn/run_mm-ss-2l3i5hp-c326464k533s212-wd1e-05-MX-tag_validation_sharpdiff.csv}\modelB
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymin=10,
        ymax=15,
        xmin=0,
        xmax=65000,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=SharpDiff,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{w/o BN (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{with BN (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-bn-psnr}
\end{subfigure}
\caption[Influences of Batch Normalization]{Comparison of the validation results from two model instances where one uses batch normalization layers in the spatial encoder and decoder components.} \label{fig:plot-mm-bn}
\end{figure}


\subsubsection*{Learning Rate Decay}

The network's learning behavior using two different exponential decay rates is illustrated in Figure \ref{fig:plot-mm-lr}. In this example, the learning rate is slightly decayed after each epoch by a specified factor. Because the Moving MNIST dataset is generated on-the-fly, we specify its total size to be virtually \num{32000}. In this way, the learning rate is effectively decayed after each validation step\footnote{In the course of this evaluation, we assess our model in a full epoch on the validation set every \nth{1000} training step.}, since a batch size of \num{32} is used. The network denoted with blue has a clear disadvantage, because its learning rate at the end became so small that it alsmost stops learning. However, throughout our experiments we determined that a very low learning rate at the end of the training was beneficial regarding the image quality and finer details of the generated images.

\begin{figure}[htb]
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/lr/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-JK-tag_validation_bce.csv}\modelA
  \pgfplotstableread{data/mm/lr/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-XG-tag_validation_bce.csv}\modelB
  \pgfplotstableread{data/mm/lr/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-JK-tag_learning_rate.csv}\modelLrA
  \pgfplotstableread{data/mm/lr/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-XG-tag_learning_rate.csv}\modelLrB
  \begin{tikzpicture}
    \begin{axis}[
    	axis y line*=right,
    	hide x axis,
    	ylabel near ticks,
        ymin=0,
        ymax=0.001,
        xmin=0,
        xmax=65000,
        thick,
        ylabel={Learning rate $\eta$},
        xlabel=step \textit{i},
        x post scale=1.8,
      ]
      \addplot[draw=black!10!blue, very thin, opacity=0.8] table[x=Step, y=Value]{\modelLrB};\label{mB}
      \addplot[draw=black!10!green, very thin, opacity=0.8] table[x=Step, y=Value]{\modelLrA};\label{mA}
    \end{axis}
    \begin{axis}[
    	axis y line*=left,
    	ymode=log,
    	log ticks with fixed point,
        ymin=0,
        ymax=0.3,
        xmin=0,
        xmax=65000,
        ytick={0.05,0.075,0.1,0.15,0.2},
        legend style={legend pos=north east},
        grid,
        thick,
        ylabel=BCE loss,
        xlabel=step \textit{i},
        x post scale=1.8,
      ]
	  \addlegendimage{/pgfplots/refstyle=mB}\addlegendentry{$\eta$-decay=0.90 (valid)}
      \addlegendimage{/pgfplots/refstyle=mA}\addlegendentry{$\eta$-decay=0.95 (valid)}
      
      %foreground
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{$\eta$-decay=0.90};
      %best
      \addplot[draw=black!30!green] table[dashed, x=Step, y=Value]{\modelA};
      \addlegendentry{$\eta$-decay=0.90};
    \end{axis}
  \end{tikzpicture}
  \caption[Influences of Learning Rate Decay]{Validation losses of two identical network models using different learning rate decay rates in combination with Adam optimizer.}\label{fig:plot-mm-lr}
\end{figure}


\subsubsection*{ConvLSTM Feature Maps}

Next, we investigate the network's behavior with respect to the hidden-to-hidden feature maps in the spatio-temporal decoder and encoder components. Therefore, we adjust the number of feature maps produced by the convolutional layers in the spatial encoder. While a first simpler variant is used that produces (16, 32, 32) feature maps in each layer, another more complex variant is used as well which convolves (64, 128, 128) feature maps. The number feature maps produced by the last layer is then used constantly within the ConvLSTM cells and its temporal transitions. Consequently, the performance of these two model configurations is compared against the standard model with 64 temporal feature maps in Figure \ref{fig:plot-mm-filter}.

\begin{figure}[htb]
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/filters/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-04-FG-tag_validation_bce.csv}\modelA
  \pgfplotstableread{data/mm/filters/run_mm-ss-2l3i5hp-c163232k533s212bn-wd1e-04-YP-tag_validation_bce.csv}\modelB
  \pgfplotstableread{data/mm/filters/run_mm-ss-2l3i5hp-c64128128k533s212bn-wd1e-05-NK-tag_validation_bce.csv}\modelC
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymode=log,
    	log ticks with fixed point,
        ymin=0,
        ymax=0.3,
        xmin=0,
        xmax=65000,
        ytick={0.05,0.075,0.1,0.15,0.2},
        legend style={legend pos=north east},
        grid,
        thick,
        ylabel=BCE loss,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{32 feature maps (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{64 feature maps (valid)};
      
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{128 feature maps (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-filter-bce}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/filters/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-04-FG-tag_validation_ssim.csv}\modelA
  \pgfplotstableread{data/mm/filters/run_mm-ss-2l3i5hp-c163232k533s212bn-wd1e-04-YP-tag_validation_ssim.csv}\modelB
  \pgfplotstableread{data/mm/filters/run_mm-ss-2l3i5hp-c64128128k533s212bn-wd1e-05-NK-tag_validation_ssim.csv}\modelC
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymin=0.5,
        ymax=1.0,
        xmin=0,
        xmax=65000,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=SSIM,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{32 feature maps};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{64 feature maps};
      
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{128 feature maps};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-filter-psnr}
\end{subfigure}
\caption[Influences of ConvLSTM Feature Maps]{Impact of the number of feature maps within the ConvLSTM encoder and decoder networks regarding overall frame prediction performance.} \label{fig:plot-mm-filter}
\end{figure}

As expected, the network with only 32 feature maps performs worse compared to the standard setting. But it is very surprising that the model configuration with 128 feature maps achieves even worse results. One possible reason could be a unfavorable, random initialization of the initial model parameters. However, due to the fact that such a network configuration requires roughly the entire video memory of the GPU, as well as takes twice as long to train, we came to the decision to continue our experiments on 64 feature maps only.


\subsubsection*{ConvLSTM Layers}

The number of recurrent layers has a very positive effect in order to generate future frames of better quality. The same behavior has also been mentioned in the practical experiment of \parencite[p. 6]{unsup_learn_lstm}, where they have qualitatively shown that a deeper LSTM is able to generate sharper images. Figure \ref{fig:plot-mm-layer} shows that the improvements of adding a third layer is apparently smaller compared to the difference when a second layer is added. Unfortunately, a 4-layer network could not be tested due to memory limitations of the graphics card. Additionally, it is worthwhile noting that the number of recurrent layers has a huge impact in respect to model complexity, training time and memory requirements.

\begin{figure}[htb] % note: these use triplet loss already!!!
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/layers/run_mm-ss-3l3i5hp-c326464k533s212bn-wd1e-05-triplet-JM-tag_validation_bce.csv}\modelA
  \pgfplotstableread{data/mm/layers/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-triplet-CE-tag_validation_bce.csv}\modelB
  \pgfplotstableread{data/mm/layers/run_mm-ss-1l3i5hp-c326464k533s212bn-wd1e-05-triplet-MK-tag_validation_bce.csv}\modelC
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
    	ymode=log,
    	log ticks with fixed point,
        ymin=0,
        ymax=0.3,
        xmin=0,
        xmax=65000,
        ytick={0.05,0.075,0.1,0.15,0.2},
        legend style={legend pos=north east},
        grid,
        thick,
        ylabel=BCE loss,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{1-layer ConvLSTM};
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{2-layer ConvLSTM};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{3-layer ConvLSTM};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-layer-bce}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/layers/run_mm-ss-3l3i5hp-c326464k533s212bn-wd1e-05-triplet-JM-tag_validation_psnr.csv}\modelA
  \pgfplotstableread{data/mm/layers/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-triplet-CE-tag_validation_psnr.csv}\modelB
  \pgfplotstableread{data/mm/layers/run_mm-ss-1l3i5hp-c326464k533s212bn-wd1e-05-triplet-MK-tag_validation_psnr.csv}\modelC
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymin=10,
        ymax=20,
        xmin=0,
        xmax=65000,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=PSNR,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{1-layer ConvLSTM};
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{2-layer ConvLSTM};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{3-layer ConvLSTM};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-layer-psnr}
\end{subfigure}
\caption[Influences of ConvLSTM Layers]{Validation performance of network models with a varying number of recurrent layers. All three models are trained using the triplet loss function.} \label{fig:plot-mm-layer}
\end{figure}



\subsubsection*{Loss Layer}

As a last experiment of our test series, we examine the impact of the different loss terms. Three networks with the identical standard configurations, but different objective functions have therefore been trained. Starting from a network using binary cross-entropy only, perceptual motivated loss terms like GDL and SSIM are added one after another until we end up in the triplet loss described in equation \ref{eq:triplet}. The validation results are depicted in Figure \ref{fig:plot-mm-loss}.

\begin{figure}[htb]
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/loss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-mae_only-HE,tag_validation_bce.csv}\modelA
  \pgfplotstableread{data/mm/loss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-JK,tag_validation_bce.csv}\modelB
  \pgfplotstableread{data/mm/loss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-triplet-CE,tag_validation_bce.csv}\modelC
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymode=log,
    	log ticks with fixed point,
        ymin=0,
        ymax=0.3,
        xmin=0,
        xmax=65000,
        legend style={legend pos=north east},
        grid,
        thick,
        ylabel=BCE loss,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{BCE+GDL+SSIM (valid)};
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{BCE+GDL (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{BCE (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-loss-bce}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/mm/loss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-mae_only-HE,tag_validation_sharpdiff.csv}\modelA
  \pgfplotstableread{data/mm/loss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-JK,tag_validation_sharpdiff.csv}\modelB
  \pgfplotstableread{data/mm/loss/run_mm-ss-2l3i5hp-c326464k533s212bn-wd1e-05-triplet-CE,tag_validation_sharpdiff.csv}\modelC
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymin=10,
        ymax=15,
        xmin=0,
        xmax=65000,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=SharpDiff,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{BCE+GDL+SSIM (valid)};
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{BCE+GDL (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{BCE (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-mm-loss-psnr}
\end{subfigure}
\caption[Influences of the Loss Term]{Comparison of the validation loss using different objective functions.} \label{fig:plot-mm-loss}
\end{figure}

While the network purely trained on the BCE loss function performs best regarding the binary cross-entropy on the validation set, it nevertheless performs worst according to the sharpness difference metric shown in Figure \ref{fig:plot-mm-loss-psnr}. These results are not surprising, because network using a the triplet loss function for example is putting more emphisis on other properties, in contrast to specifically optimizing it regarding to the binary cross-entropy. However, after we have qualitatively analyzed many generated prediction samples in the course of our experiments, we had the impression that results using the simple BCE loss produced the best results. Especially in samples where one handwritten number crosses the other number, the predictions multiple time steps afterwards kept slightly more stable. All in all, perceptual motivated loss terms seem to be useless or even slightly counter-productive when applied on binary Moving MNIST image frames. 

\subsection{Results}

For the final evaluation of our model with Moving MNIST, we assess the qualitativ and quantitative performance of the best configuration on the test set. Inspired by the findings of our model exploration, as well a rudimentary grid-search, the model for this final test ends up to use the default settings as described earlier, but with an exponential learning rate decay of $\alpha=0.95$ and a loss layer that utilizes no perceptual motivated loss term. We have trained three models of this network with different numbers of recurrent layers for \num{100000} steps.


\subsubsection{Quantitative Results}

A comparison of our best performing model with experimental results of other works is given in Table \ref{tab:mm-comparison}. It can be seen that our model exhibits way less model parameters, especially compared to FC-LSTM approaches. Nevertheless, our ConvLSTM model is able to cut the average pixel-wise test error of the best performing competitive model in halves. The listed results are taken from several published papers. However, we were not able to reproduce the results of FC-LSTM-Combo published in \parencite{unsup_learn_lstm} using their open-source code. After training the model for about \num{1000000} training steps, which took almost one week, the results were not nearly as good as in the paper. Even about \num{20000} training iterations are already sufficient for our model to generate better results.

\begin{table}[htb]
  \small
  \centering
  \begin{tabular}{l r r}
    \toprule
      \textbf{Model} & \textbf{Trainable parameters} & \textbf{BCE test error} \\
    \midrule
      FC-LSTM-Combo(2048-2048) \tiny{\parencite{unsup_learn_lstm}} & \num{214001664} & 0.0855 \\
      FC-LSTM(2048-2048) \tiny{\parencite{conv_lstm_nowcasting}} & \num{142667776} & 0.1180 \\
      ConvLSTM(5/128-64-64) \tiny{\parencite{conv_lstm_nowcasting}} & \num{7585296} & 0.0896 \\
      2enc-ConvLSTM(7/45-45)\footnote{Single frame prediction result, not the average error ofall 10 predicted future frames.} \tiny{\parencite{spat_temp_video_autoenc}} & \num{6132203} & 0.0835 \\
    \midrule
      3enc-ConvLSTM-SS-3dec(5/64) & \num{1841793} & 0.0524 \\
      3enc-ConvLSTM-SS-3dec(5/64-64) & \num{3570817} & 0.0414 \\
      3enc-ConvLSTM-SS-3dec(5/64-64-64) & \num{1} & \\ %TODO
    \bottomrule
  \end{tabular}
  \caption[Results on Moving MNIST]{Comparison with other networks on Moving MNIST. The numbers in brackets identify the \textit{amount of hidden units} per layer in case of FC-LSTMs, and for ConvLSTMs the \textit{hidden-to-hidden kernel size} followed by the \textit{feature maps per layer} are listed. Further, \textit{3enc} denotes tree convolutional layers in the spatial encoder. We call our model 3enc-ConvLSTM-SS-3dec.}\label{tab:mm-comparison}
\end{table}

\subsubsection{Qualitative Results}

Next, several future frame prediction samples are qualitatively compared with results of other models presented earlier in chapter \ref{chapter:relatedwork}. To that end, we take all Moving MNIST examples that are published in other works and apply our network on exactly the same input data. The contrast against an FC-LSTM approach can be seen in Figures \ref{fig:mm-pred-spec-lstm1} or \ref{fig:mm-pred-spec-lstm1}, as well as the comparison with another ConvLSTM implementation in Figure \ref{fig:mm-pred-spec2-clstm1}. The outcomes of our two-layer model are slightly sharper and much brighter in all three examples. However, the digits \num{3} and \num{0} in Figure \ref{fig:mm-pred-spec-lstm} show some visible artifacts at the end of the predicted future sequence.

\begin{figure}[h!tb]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/mm/spec/prediction-00.png}
  \caption{}
  \label{fig:mm-pred-spec-lstm1}
\end{subfigure}%
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/mm/spec/prediction-01.png}
  \caption{}
  \label{fig:mm-pred-spec-lstm2}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/mm/spec/prediction-02.png}
  \caption{}
  \label{fig:mm-pred-spec-lstm3}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/mm/spec/prediction-03.png}
  \caption{}
  \label{fig:mm-pred-spec-lstm4}
\end{subfigure}
\caption[Comparison with LSTM on Moving MNIST]{Qualitative comparison with FC-LSTM-Combo(2048-2048) \parencite{unsup_learn_lstm}. Experiments in (c) and (d) are using a different number of digits than the network was trained for. From top to bottom: inputs sequence; ground truth target sequence; predictions of FC-LSTM-Combo; predictions of our 2-layer model.} \label{fig:mm-pred-spec-lstm}
\end{figure}

Further predictions of our model, which have been randomly chosen from the Moving MNIST test set, can be found in Figure \ref{fig:mm-pred-random} in the appendix. While most generated results look quite similar to the ground truth future, the sample in Figure \ref{fig:mm-pred-random2} clearly shows that the model still has its issues to preserve details when both digits overlap each other over a longer period.


\subsubsection{Out-of-Domain Results}

Additionally, we verify the model's ability to deal with data that is outside the domain it has been trained for. We therefore take several Moving MNIST sequences that use one or three digits instead of two and compare the generated results with predictions generated by other competitive models. At this point, it is to highlight that we are not just cherry-picking the best generated predictions in this experiment, because we take the exact same sequences that have been used in related works. In a manner of speaking, we are probably using input sequences where other network models have performed best.

\begin{figure}[h!tb]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/mm/spec/prediction-04.png}
  \caption{}
  \label{fig:mm-pred-spec2-clstm1}
\end{subfigure}%
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/mm/spec/prediction-05.png}
  \caption{}
  \label{fig:mm-pred-spec2-clstm2}
\end{subfigure}
\caption[Comparison with other ConvLSTM on Moving MNIST]{Qualitative comparison with ConvLSTM(5/128-64-64 \parencite{conv_lstm_nowcasting}. The experiment in (b) uses more digits than the network was trained for. From top to bottom: inputs sequence; ground truth target sequence; prediction results of the competitive model; predictions of our 2-layer ConvLSTM.} \label{fig:mm-pred-spec2-clstm}
\end{figure}

A remarkable difference is visible when the outcames of our model is qualitatively compared with predictions of existing approaches. In case of the samples using three digits in Figures \ref{fig:mm-pred-spec-lstm4} and \ref{fig:mm-pred-spec2-clstm2}, the structure of each digit stays more stable and does not get washed-out, even after a digit has been overlapped by both others. Moreover, it does not try to merge two numbers in order to end up with the same number of digits like the networks have been trained for. A similar behaviour can be seen in Figure \ref{fig:mm-pred-spec-lstm3}, where only a single digit is used. In that scenario, the other network model cearly tries to hallucinate a second fyling number at the end of the future sequence. This misbehavior is not visible in the outputs of our model.

Futher out-of-domain results can be found in Figure \ref{fig:mm-pred-long} in the appendix. It shows the scenario where our model tries to predict a much longer sequence than it has been trained for. Such a scenario is very to realize with our model due to its recurrent encoder-decoder architecture. We therefore unroll the spatio-temporal decoder component for \num{30} time steps in order to verify the long-term stability of the predicted frames. It can be seen that the network is able to continuously produce acceptable results, with the exception of the last five frame in Figure \ref{fig:mm-pred-long1}. From about the \nth{30} predicted future frame, our model then starts to produce very noisy outcomes in almost every case.






























\section{Experiments on MsPacman} \label{sec:exp-pac}

The network model is applied on the MsPacman dataset in our second experiment to see how it behaves with data of increased complexity. The effects of perceptual motivated loss terms are investigated more deeply in this dataset, because we strongly belief that their weaknesses in Section \ref{sec:exp-mm} are cause by the unnaturally of binary image data.


\subsection{Hyperparamter Tuning}

In order to find good hyperparameters for the network model, we started to use the best performing model of Moving MNIST, but with the four modifications. First, the three layers of the spatial encoder and decoder components are using strides of $s_i=(1, 2, 1)$ instead $s_i=(2, 1, 2)$, where $i$ denotes the index of the convolutional layer, in order to end up with a feature space representation of the same shape as before. This change is is caused by the fact that the model is trained on smaller $32 \times 32$ patches. Second, we use a \textit{tanh} activation function in the output layer so that each generated frame is within the valid scale. Third, a learning rate decay of $\alpha=0.95$ per epoch is used by default. And lastly, the triplet loss function of Section \ref{sec:impl-components} with MAE as the main loss function is used as a starting point in this evaluation, because previous works have shown its advantages in image processing tasks compared to squared error.

We trained several networks with different numbers of recurrent layers, varying initial learning rates $\eta \in \{0.0005, 0.001, 0.005\}$, as well as different weight decays $\lambda \in \{1e^{-6}, 1e^{-5}, 1e^{-4}\} $. While we could see a clear imrovement between the single layer and two-layer ConvLSTM model, there has been no clear benefit from using a third layer. The differences in results when we performed variations in learning rate and weight decay have also been marginal. Eventaully, we ended up with an initial learning rate of $\eta = 0.0005 $ and continued to use the default weight decay of $\lambda = 1e^{-5}$. Further, we used the two-layer model with the main reason to save training time.


\subsubsection*{Loss Layer}

Even that we couldn't improve our network's performance by performing variations in several hyperparameters that much, we experienced that variations in the objective function could caused more noticeable differences in both quantitativ and qualitative results. As a consequence, we descided to do a further investigation by using different loss terms. Severals validation results are therefore illustrated in Figure \ref{fig:plot-pac-loss}. 

\begin{figure}[htb]
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/pac/loss/mse/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mse-triplet-MZ_validation_mse.csv}\modelA
  \pgfplotstableread{data/pac/loss/mse/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mse-QQ_validation_mse.csv}\modelB
  \pgfplotstableread{data/pac/loss/mse/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mae-triplet-VY_validation_mse.csv}\modelC
  \pgfplotstableread{data/pac/loss/mse/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mae-LU_validation_mse.csv}\modelD
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymode=log,
    	log ticks with fixed point,
        ymin=0,
        ymax=0.3,
        xmin=0,
        xmax=100000,
        legend style={legend pos=north east},
        grid,
        thick,
        ylabel=MSE loss,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!orange] table[x=Step, y=Value]{\modelD};
      \addlegendentry{MAE+GDL (valid)};
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{MAE+GDL+SSIM (valid)};
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{MSE+GDL (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{MSE+GDL+SSIM (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-pac-loss-mse}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/pac/loss/mae/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mse-triplet-MZ_validation_mae.csv}\modelA
  \pgfplotstableread{data/pac/loss/mae/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mse-QQ_validation_mae.csv}\modelB
  \pgfplotstableread{data/pac/loss/mae/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mae-triplet-VY_validation_mae.csv}\modelC
  \pgfplotstableread{data/pac/loss/mae/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mae-LU_validation_mae.csv}\modelD
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
        ymode=log,
    	log ticks with fixed point,
        ymin=0,
        ymax=0.3,
        xmin=0,
        xmax=100000,
        legend style={legend pos=north east},
        grid,
        thick,
        ylabel=MAE loss,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!orange] table[x=Step, y=Value]{\modelD};
      \addlegendentry{MAE+GDL (valid)};
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{MAE+GDL+SSIM (valid)};
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{MSE+GDL (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{MSE+GDL+SSIM (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-pac-loss-mae}
\end{subfigure}
\caption[Comparison of Losses on MsPacman]{Comparison of validation results on MsPacman using a 2-layer network with varying loss function combinations. All models have been trained given an input sequence of 8 patches to predict the next 8 frames. The loss values are shown in the graphs represent the per-pixel error.} \label{fig:plot-pac-loss}
\end{figure}

Boths diagrams show that the use of the triplet loss function leads to better results regarding squared or absolute error. The same is true for the SSIM index, as it can be seen in Figure \ref{fig:plot-pac-loss-ssim}. But overall, the MSE loss function seams to results in a slightly better network performance. Special emphasis should be placed on the fact that for example a network that is trained using the triplet loss MSE+GDL+SSIM performs even better than a model trained with MAE+GDL regarding the MAE loss on the validation set, even that the latter network is explicitely optimized for this objective. Additionally, according the sharpness metric demonstrated in Figure \ref{fig:plot-pac-loss-sdiff}, the predicted frames of both networks that utilize the mean squared error as its main loss function feature sharper edges.

\begin{figure}[htb]
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/pac/loss/sdiff/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mse-triplet-MZ_validation_sharpdiff.csv}\modelA
  \pgfplotstableread{data/pac/loss/sdiff/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mse-QQ_validation_sharpdiff.csv}\modelB
  \pgfplotstableread{data/pac/loss/sdiff/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mae-triplet-VY_validation_sharpdiff.csv}\modelC
  \pgfplotstableread{data/pac/loss/sdiff/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mae-LU_validation_sharpdiff.csv}\modelD
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
    	log ticks with fixed point,
        ymin=10,
        ymax=23,
        xmin=0,
        xmax=65000,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=SharpDiff,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!orange] table[x=Step, y=Value]{\modelD};
      \addlegendentry{MAE+GDL (valid)};
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{MAE+GDL+SSIM (valid)};
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{MSE+GDL (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{MSE+GDL+SSIM (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-pac-loss-sdiff}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \pgfplotstableset{col sep=comma}
  \pgfplotstableread{data/pac/loss/ssim/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mse-triplet-MZ_validation_ssim.csv}\modelA
  \pgfplotstableread{data/pac/loss/ssim/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mse-QQ_validation_ssim.csv}\modelB
  \pgfplotstableread{data/pac/loss/ssim/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mae-triplet-VY_validation_ssim.csv}\modelC
  \pgfplotstableread{data/pac/loss/ssim/run_pac-ss-2l3i5hp-c326464k533s121bn-wd1e-05-mae-LU_validation_ssim.csv}\modelD
  \hspace*{-0.6cm}
  {
  \begin{tikzpicture}[scale=0.5]
    \begin{axis}[
    	log ticks with fixed point,
        ymin=0.5,
        ymax=1.0,
        xmin=0,
        xmax=65000,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=SSIM,
        xlabel=step \textit{i},
        x post scale=1.6,
      ]
      \addplot[draw=black!30!orange] table[x=Step, y=Value]{\modelD};
      \addlegendentry{MAE+GDL (valid)};
      \addplot[draw=black!30!red] table[x=Step, y=Value]{\modelC};
      \addlegendentry{MAE+GDL+SSIM (valid)};
      \addplot[draw=black!30!blue] table[x=Step, y=Value]{\modelB};
      \addlegendentry{MSE+GDL (valid)};
      %best
      \addplot[draw=black!30!green] table[x=Step, y=Value]{\modelA};
      \addlegendentry{MSE+GDL+SSIM (valid)};
    \end{axis}
  \end{tikzpicture}
  }
  \caption{}
  \label{fig:plot-pac-loss-ssim}
\end{subfigure}
\caption[Comparison of Image Metrics on MsPacman]{Image metrics computed after each training epoch on the MsPacman dataset. All four networks use a 2-layer ConvLSTM and the same hyperparameters, but a different objective function.} \label{fig:plot-pac-imgmetric}
\end{figure}

As a final qualitative comparison regading the differences of these objective function, we present two samples of the best and worst performing model in Figure \ref{fig:pac-pred-blurry}. While the network that has generated Figure \ref{fig:pac-pred-sharp_sample} is trained using the MSE-based triplet loss function, so is the other model instance, which has predicted the frames of the third column in Figure \ref{fig:pac-pred-blurry_sample}, trained on a combined MAE+GDL function. By comparing both predictions of this similar scene, the ghost of the latter network does visibly lose its finer details. More precisely, is is predicted as a simple blob without its eyes. 

\begin{figure}[h!tb]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/pac/blurry/pred-sharp.png}
  \caption{}
  \label{fig:pac-pred-sharp_sample}
\end{subfigure}%
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/pac/blurry/pred-blurry.png}
  \caption{}
  \label{fig:pac-pred-blurry_sample}
\end{subfigure}
\caption[Generated MsPacman Patches using Different Loss Functions]{Two predictions showing a similar scene of a moving ghost. The sequence in (a) is predicted using a network trained with the MSE-based triplet loss function, whereas the network in (b) is trained on MAE+GDL loss.} \label{fig:pac-pred-blurry}
\end{figure}


\subsection{Results}

Next, we present the test results of our best performing two-layer model. This model is configured as described earlier in this section. In the qualitative assessments, our network takes advantage from the triplet loss function using MSE as its main loss term which produced the most promising results when we explored the behavior of the model on the MsPacman dataset. It is important to note that the test set is not filtering out random crops that show just few or even no movement at all. That's why it is to expect that the average test results can be much better compared to the validation result, since it is quite easy for the network to predict scenes that only contain the maze's static background with no movement at all.


\subsubsection{Quantitative Results}

The image metric results of our network model using the discussed loss function combinations is shown in Table \ref{tab:pac-comparison}. Unfortunately, there is no related project that has published any future frame prediction results on this dataset yet. However, this dataset is indeed used in an independent implementation of \parencite{deep_multiscale_video_pred}. We added the result of some experiments with it for the sake of completion, but these results are hard to compare with ours, because the metrics are calculated on that parts within a frame where motion is actually happening. Additionally, it predicts only one frame given a sequence of four.

As it has been the case on the validation set, the MSE-based triplet loss function produces the best results in all similarity metrics, regardless of it is about the first predicted frame, or the average of the whole future sequence. However, if we look at the sharpness results, the model instance that utilizes no SSIM term in its objective function is able to produce sharper results according to the metric. This is not surprising, because the GDL term is weighted more in this model, which is the central objective of this metric.

\begin{table}[htb]
  \footnotesize
  \centering
  \begin{tabular}{l | r r | r | r r | r}
    \toprule
      \textbf{Loss function} & \multicolumn{3}{c}{\textbf{\nth{1} frame}} & \multicolumn{3}{c}{\textbf{mean of 8 frames}} \\
      & \multicolumn{2}{c}{\scriptsize{Similarity}} & \scriptsize{Sharpness} & \multicolumn{2}{c}{\scriptsize{Similarity}} & \scriptsize{Sharpness} \\
      & PSNR & $\text{SSIM}_{5}$ & SharpDiff & PSNR & $\text{SSIM}_{5}$ & SharpDiff \\
    \midrule
      Adv+GDL \tiny{\parencite{tf_impl_gan}} & 25.3168 & - & 17.2725 & - & - & \\
    \midrule
      MAE+GDL & 47.5462 & 0.9854 & 27.0217 & 48.6902 & 0.9817 & 27.8182 \\
      MAE+GDL+SSIM & 44.8974 & 0.9917 & 26.9131 & 44.6555 & 0.9856 & 27.9516 \\
      MSE+GDL & 50.5197 & 0.9893 & \textbf{30.0961} & 50.1164 & 0.9833 & \textbf{29.9851} \\
      MSE+GDL+SSIM & \textbf{50.4655} & \textbf{0.9925} & 28.5023 & \textbf{50.2746} & \textbf{0.9862} & 28.7008 \\
    \bottomrule
  \end{tabular}
  \caption[Metric Results on MsPacman]{Metric results of produced patch sequences using a 2-layer model on MsPacman's test split. The achieved similarity and sharpness of the same models using different loss layers are given for both the first frame only and the average of the whole generated future sequence. The results of the other approach in the first row are not camparable to ours due to a different evaluation procedure.}\label{tab:pac-comparison}
\end{table}

Another interesting fact is that the differences between the image metric results of the first predicted frame and the average of all predicted frames are very marginal. This might be due to the reason that the bigger part of each patch is just static, noise-free content. Hence, the actual interesting parts of each frame that include motion have only little effect on the metric results. Furthermore, it is surprising that the sharpness metric results of the first predicted frame are slightly worse compared to the metric's mean over the whole predicted sequence. 

On the other side, the pixel-wise absolute and squared error results in Table \ref{tab:pac-comparison2} appear more plausible. Here, the differences between the frist frame and the average of the sequence are significantly larger. But one fact remains unchanged: the networks trained on the MSE-based triplet loss function is able to produce the best prediction results. Furthermore, it is interesting to see that both networks that utilize a squared error loss term as their main objective are able to outperform both other model instances regarding the MAE error, despite the fact the other who networks are explicitely optimized for this for this objective. This superiority could be reasoned by the characteristics of the MsPacman dataset.

\begin{table}[htb]
  \footnotesize
  \centering
  \begin{tabular}{l | r r | r r }
    \toprule
      \textbf{Loss function} & \multicolumn{2}{c}{\textbf{\nth{1} frame}} & \multicolumn{2}{c}{\textbf{mean of 8 frames}} \\
      & MAE & MSE & MAE & MSE \\
    \midrule
      MAE+GDL & 0.0033 & 0.0009 & 0.0041 & 0.0018 \\
      MAE+GDL+SSIM & 0.0031 & \textbf{0.0005} & 0.0047 & 0.0015 \\
      MSE+GDL & 0.0025 & 0.0006 & 0.0040 & 0.0016 \\
      MSE+GDL+SSIM & \textbf{0.0023} & \textbf{0.0005} & \textbf{0.0037} & \textbf{0.0014} \\
    \bottomrule
  \end{tabular}
  \caption[Test Errors on MsPacman]{Absolute and squared error test results on MsPacman dataset using varying loss layers in our 2-layer network.}\label{tab:pac-comparison2}
\end{table}


\subsubsection{Qualitative Results}

For a qualitative evaluation of the network, we take a detailed look at some generated future sequences. Starting the the clip in Figure \ref{fig:pac-pred-random1}, the motion of the ghost is predicted very precisely. Moreover, the fine details of the ghost's eyes and zigzag mouth are preserved even until the eighth frame. However, the dynamics of the blue and white blinking effect is not continued in the predicted sequence.

\begin{figure}[h!tb]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/pac/random/pred-00.png}
  \caption{}
  \label{fig:pac-pred-random1}
\end{subfigure}%
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=0.92\linewidth]{figures/pred/pac/random/pred-04.png}
  \caption{}
  \label{fig:pac-pred-random2}
\end{subfigure}
\caption[Random Prediction Samples on MsPacman]{Two random prediction samples on MsPacman that contain at least some motion. From top to bottom: ground truth input and output sequence; predicted sequence of the 2-layer network.} \label{fig:pac-pred-random}
\end{figure}

As a second example, the prediction result shown in Figure \ref{fig:pac-pred-random2} is considered. On the one side, pacman's dynamics are correctly continued in first four generated frames. It correctly moves forward, performs chewing and eats the orange dot, but with some delay. On the other side, the game character suddenly disappears when it arrives the second crossing. We observed such a behavior actually quite often in many other samples as well. As a results, this indicates that the network has its issues to decide what direction the moving objects will take within the maze. This assumption can be further confirmed by some other other observations, such as that objects reaching a crossing are either affected by a loss of opacity, split up into multiple directions, disapper complete as shown in the demonstrated example, or in best case chose any random direction. Moreover, such disappearance effects seldomly take place in corners where there is beside turning around only one other option.

More predictions on small patches can be found in Figure \ref{fig:pac-pred-random_extra} in the appendix. It is worth noting that still-standing motion is particularly well captured and continued by the network. As an example, pacman's chewing animation in Figure \ref{fig:pac-pred-random_extra1} is kept up in all predicted frames, so that it is looks almost identical at the first glance. The only noticeable difference here is that pacman temporarily changes its direction in the fifth frame for just one time step in the ground truth future.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/pred/pac/full/pred-00.png} 
	\caption[Random Full Screen Prediction Sample on MsPacman]{Prediction sample of a full game scene using our 2-layer network, which is trained on small $32 \times 32$ patches only. This is possible due to our FCN approach. From top to bottom: ground truth input and output sequence; predicted game sequence.} \label{fig:pac-pred-full1}
\end{figure}

As explained in section \ref{sec:fcn}, the fully-convolutional approach of our model comes with the advantage that it can be applied on input images of larger size. Therefore, a prediction sample of a sequence showing a full game scene is presented in Figure \ref{fig:pac-pred-full1}. It shows that future frames can indeed be forecasted for the full images with acceptable results as well. For instance, the blinking effect of the large dots in the corners of the maze and the trajectory of the red cherry is predicted almost perfectly. Furthermore, the motion of the game characters is continued quite well, with the exception one ghost that disappears or that both ghosts in the center slightly change their color. A second result showing the full maze can be found in Figure \ref{fig:pac-pred-full2}. in that sample, attention should be paid to the blue ghost in the center, who is correctly predicted to leave the cave instead of going for another spin.

Finally, the stability of the generated future frames investigated by performing predictions on longer sequences. As it can be seen in Figure \ref{fig:pac-pred-full-long}, some artifacts become visible starting form the \nth{14} forecasted frame. And while all moving game characters gradually disappear, the red cherry object is predicted very precisely until the end of the lengthened sequence.













\subsection{Experiments on UCF-101}


\begin{table}[htb]
  \footnotesize
  \centering
  \begin{tabular}{l | r r | r | r r | r}
    \toprule
      \textbf{Loss function} & \multicolumn{3}{c}{\textbf{\nth{1} frame}} & \multicolumn{3}{c}{\textbf{sequence avg.}} \\
      & \multicolumn{2}{c}{\scriptsize{Similarity}} & \scriptsize{Sharpness} & \multicolumn{2}{c}{\scriptsize{Similarity}} & \scriptsize{Sharpness} \\
      & PSNR & SSIM & & PSNR & SSIM & \\
    \midrule
      single-scale $\ell_2$ \tiny{\parencite{deep_multiscale_video_pred}} & 26.5 & 0.84 & 24.7 & - & - & - \\
      multi-scale $\ell_1$ \tiny{\parencite{deep_multiscale_video_pred}} & 28.7 & 0.88 & 24.8 & - & - & - \\
      multi-scale GDL+$\ell_1$ \tiny{\parencite{deep_multiscale_video_pred}} & 29.4 & 0.90 & 25.0 & - & - & - \\
      multi-scale Adv+GDL \tiny{\parencite{deep_multiscale_video_pred}} & 31.5 & 0.91 & 25.4 & - & - & - \\
    \midrule
      MAE+GDL & psnr & ssim & sharp & psnr & ssim & sharp \\
      MAE+GDL+SSIM & psnr & ssim & sharp & psnr & ssim & sharp \\
      MSE+GDL & psnr & ssim & sharp & psnr & ssim & sharp \\
      MSE+GDL+SSIM & psnr & ssim & sharp & psnr & ssim & sharp \\
    \bottomrule
  \end{tabular}
  \caption[Metric Results on UCF-101]{Metric results of predicted patch sequences using our approach on the UCF-101 test split. The achieved similarity and sharpness results by using different loss layers are given for both the first frame only and the mean of the generated sequence. The results of both listed appraches are not comparable, because the other one takes only the moving areas of the images into account.}\label{tab:ucf-comparison}
\end{table}



%\subsubsection{Visualization of Learned Motion Representation} ??? maybe at least in the attachements?



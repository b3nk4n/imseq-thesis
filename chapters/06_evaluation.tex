% !TeX root = ../main.tex

% for every dataset-group: tell which loss_main was used by default (if not stated differently)
% tell we use batch_size of 32 always (without any reason...)

\chapter{Evaluation} \label{chapter:evaluation}

% performed on: We implemented our model in TensorFlow v0.10, CUDA 7.5.18 and cuDDN 4.

% explore the model on MovingMNIST (layers, scheduled samling, filter depth)
%      --> fix the main hyper-params
% small grid search on each model?
%      --> compare results

\section{Training / Training Procedure}

Wie gehen wir beim Training vor: Wegen sehr vielen Hyperparmeter: Nutzen MM um die einzelnen HParams unanbh√§ngig voneinander zu erforschen. Dann: Grid-Search auf den jeweils besten Einstellungen


\subsection{Influence of Scheduled Sampling}

\subsection{Influence of LSTM-Layers}

\subsection{Influence of LSTM Filter-Depth}

\subsection{Model Selection Strategy}

Diagram: How did we pic our final model?
Seperate for each dataset?


\section{Results}

\subsection{Experiments on MovingMNIST}

\subsubsection{Hyperparameters}

Include a comparison to Other papers (using same images!)

\subsubsection{Prediction Samples}

\subsubsection{Transfer Learning}

1 char / 3 char images

\subsubsection{Visualization of Learned Motion Representation}


\subsection{Experiments on UCF-101}

\subsubsection{Hyperparameters}

\subsubsection{Prediction Samples}


\subsection{Experiments on MsPacman}

\subsubsection{Hyperparameters}

\subsubsection{Prediction Samples}







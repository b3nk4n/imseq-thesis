% !TeX root = ../main.tex


\chapter{Contribution} \label{chapter:contribution}

Before concluding this thesis, we would like to present a side contribution that arised in parallel to this thesis. When we started to work on our implementation, the TensorFlow library for machine intelligence had just published its second public release with version \num{0.7}. Thus, there has not been that much experience and best practices around with TensorFlow, as well as its API is still very low-level for several use cases even today. As a result, there was the desire to create a reusable library to reduce boilerplate code of TensorFlow based projects, as well as to retain best practices of existing examples and also this thesis. A second idea was that future theses or other deep learning projects of the \textit{Computer Visoin Group}\footnote{Computer Vision Group at Technische Universität München: \url{https://vision.in.tum.de/}} at TUM might benefit from such a library. However, this project has grown larger and larger over time and ended up in a powerful high-level framework, that has been developed independently from other high-level APIs for TensorFlow like \textit{TF-Slim}\footnote{TFLearn - Deep learning library featuring a higher-level API for TensorFlow: \url{http://tflearn.org/}} or \textit{Keras}\footnote{Keras - Deep learning library for Theano and TensorFlow: \url{https://keras.io/}}. Ultimately, about $ 99\% $ of the overall code of this thesis has been transferred into this framework, throughout with having abstraction and reusability in mind.

% features:
%     - Provides convenient functions to simplify existing TensorFlow functions
%     - Provides convenient functions for runtime summaries on TensorBoard, such as summaries of convolutional filters, loss summaries, weight histograms, etc.
% utils: python/numpy related conv. functions: video, image, data, ...
% visualizations/ui: in context to work also when used in ipyhton notebook

\section{A High-Level Framework for TensorFlow}

In this section, we present the design goals and key features of the TensorLight\footnote{TensorLight - A lightweight, high-level framework for TensorFlow:\\ \url{https://github.com/bsautermeister/tensorlight}} framework for TensorFlow based projects. Additionally, we visualize the main pricipal architecture and demonstrate its usage in a short example. We make the code of the project available to the research community under the MIT license.

\subsection{Guiding Principles}

The TensorLight framework is developed under four core principles, namely \textit{simplicity}, \textit{compactness}, \textit{standardization} and \textit{superiority}. These goals are beriefly described in the following list:

\begin{itemize}
\item \textbf{Simplicity:} Straigt-forward to use for anybody who has already worked with TensorFlow. Especially, no further learning is required regarding how to define a model's graph definition.
\item \textbf{Compactness:} Reduce boilerplate code, while keeping the transparency and flexibility of TensorFlow. 
\item \textbf{Standardization:} Provide a standard way in respect to the implementation of models and and datasets in order to save time. Further, it automates the whole training and validation process, but also provides hooks to maintain customizability.
\item \textbf{Superiority:} Enable advanced features that are not included in the TensorFlow API, as well as retain its full functionality.
\end{itemize}


\subsection{Key Features}

To highlight the advanced features of TensorFlow, we would like to provide an incomplete list of the ten main functionalities that are not shipped with TensorFlow out-of-the-box. Some of them might even be missing in other high-level APIs. These include:

\begin{itemize}
\item Transparent lifecycle management of the session and graph definition.
\item Train or evaluate a model with a single line of code.
\item Abstracted, runtime-exchangeable input pipelines which either use the simple feeding mechanism with numpy arrays, or even multi-threaded input queues.
\item Automatic saving and loading of hyperparamters as JSON to simplify the evaluation management of numerous trainings.
\item Effortless support to train a model symmetrically on multiple GPUs.
\item Ready-to-use loss functions and metrics, even with latest advances for perceptual motivated image similarity assessment.
\item Abstract classes for models, datasets and optimizers to provide a reusable plug-and-play support.
\item Ability to work with other higher-level libraries hand in hand, such as \textit{tf.contrib} or \textit{TF-slim}.
\item Automatic creation of periodic checkpoints and TensorBoard summaries
\item Extended recurrent functions to enable scheduled sampling, as well as an implementation of a ConvLSTM cell.
\end{itemize}


\subsection{Architecture}


\subsection{Example}


Model code \ref{code:model}.



\begin{figure}[htpb]
  \begin{tabular}{c}
  \begin{lstlisting}[language=Python]
	rt = tt.core.MultiGpuRuntime("/tmp/train", gpu_devices=[0, 1])
	rt.register_model(SimpleAutoencoderModel(5e-4))
	rt.register_optimizer(tt.training.Optimizer(tt.training.ADAM,
	                                            0.001, 1000, 0.95))	
	rt.register_datasets(tt.datasets.mnist.MNISTTrainDataset(),
	                     tt.datasets.mnist.MNISTValidDataset())
	rt.build(is_autoencoder=True)
	rt.train(batch_size=128, epochs=100)
  \end{lstlisting}
  \end{tabular}
  \caption[Example listing]{An example for a source code listing.}\label{code:runtime}
\end{figure}

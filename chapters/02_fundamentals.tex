% !TeX root = ../main.tex

\chapter{Fundamentals} \label{chapter:fundamentals}

\section{Learning}

What is unsupervised learning in comparison to supervised (or reinforcement leanring?)
Or just a general topic about Leanring with subsections Unsupervised, Supervised, Reinforcement?

\subsection{Supervised Learning}
\subsection{Reinforcement Learning}
\subsection{Unsupervised Learning}


\section{Neural networks}

History as short intro, etc...

\subsection{Basics}

Short intro to perceptron, neuron...

\subsection{Learning and Optimization Problem}

Gradient descent, what is actually optimized, short intro to back-prop?

\subsubsection{Loss Function}

Example of a loss function, how it is used in optimization?

\subsubsection{Gradiend Descent}

Explain GD, why SGD is used. And also introduce Adam here already?


\subsection{Activation}

Short description of different activation functions


\section{Convolutional Neural Networks}

Networks for spatial learning...

\subsubsection{History}
Short history as intro LeCun, OCR, ...

\subsection{Benefits}

(1) Param sharing, (2) sparsity, (3) ...

\subsection{Convolutional Operation}

Explain the conv op

\subsection{Transposed Convolutional Operation}

What is the conv-t op. Foot comment: Should not be called deconv

\subsection{Fully-convolutional Networks}

What are FCNs and what is their benefit?


\section{Recurrent Neural Networks}

Networks for temporal learning...

\subsection{RNN}

Bla bla...

\subsubsection{History}
Short intro (history) to RNNs

\subsubsection{Architecture}
explain it in with a small diagram

\subsubsection{BPTT}
What is BPTT in comparision to back propagation

\subsubsection{Drawbacks}
What are the drawbacks of RNNs: vanishing/explosion of gradients problem

\subsection{LSTM}

Bla bla

\subsubsection{History}
Short intro (history) to LSTMs (Hochreiter, Schmidhuber)

\subsubsection{Architecture}
Show differences to classical RNNs

\subsubsection{Benefits}
How does this suspensate the drawbacks of RNNs (gates, ...)


\section{Autoencoder Networks}

What are autoencoder networks?


\section{Parameter initialization}

Explain why they are randomly initialized. Why xavier method? How is xavier-init defined?


\section{Batch Normalization}


\section{Perceptuall Similarity of Images}

Introduce perceptual similarity and explain some metrics (SSIM, PSNR, GDL?) which we will use in evaluation.
Why is perceptuall loss so important? Example of chess/zebra image?


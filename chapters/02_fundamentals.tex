% !TeX root = ../main.tex

\chapter{Fundamentals} \label{chapter:fundamentals}

To get a general understanding of how training a neural network works, we have to go through its theoretical concepts first. We start with the structure of simple feed-forward networks, continue with advanced model architectures that take advantage of the data's spatial or temporal properties, and finally end up with recent enhancements that we use throughout our final implementation.


\section{Neural Networks}

The main concept of neural networks (NN) dates back to the early 1950s, Warren McCulloch and Walter Pitts tried to build a mathematical model of information processing in our brain. Inspired by this work, Frank Rosenblatt developed the so called \textit{perceptron} about two decades later \parencite[p. 226]{pattern_and_ml}. 

\subsection{Basics}

The perceptron itself is has quite a simple structure. It is usually visualized as a node that consists any number of binary inputs $ x_{i} $, as well as a single output $ y $ with $ x_{i}, y \in \{0, 1\} $. In addition, each input is weighted by $ w_{i} \in \mathbb{R} $ to express importance of each particular input. The output is determined by the simple rule that the weighted sum of all inputs has to reach a specified threshold to make the perceptron fire its output \parencite{neural_nets_deep_learning}. Usually, this threshold is usually called bias $ b \in \mathbb{R} $, defined as the negative threshold. All of this can be expressed as follows:

\begin{equation} \label{eq:mlp}
  y = \begin{cases}
    1, & \text{if $ \sum\limits_{i=1}^n \, w_{i}x_{i} + b > 0$},\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}

Even that its formulation is that simple, it can represent complex decision-making when we stack multiple elements togther, known as multilayer perceptrons (MLP). Such a network forms an directed acyclic graph (DAG) and is illustrated in figure \ref{fig:mlp}.

\begin{figure}[htpb]
	\centering
	\includegraphics[scale=0.8]{figures/mlp.pdf}
	\caption[Multilayer perceptron]{Example of a MLP. A single perceptron is highlighted in bold.} \label{fig:mlp}
\end{figure}

The first and last layer of such a network are refered as \textit{input layer} and \textit{output layer} and their number is determined by the given problem to solve. In case we want to train a network that identifies human faces in colored pictures with height and width of 100 pixels, it would require our input layer to have $ n_{input}=3000 $ perceptrons, as well as a single output. In contrary, all intermediate layers are known as \textit{hidden layers} and can have any number of elements and depth. When every node from one layer is connected to all nodes of its subsequent layer, we call it \textit{fully connected} (FC).

Afterwards, we can feed the input layer with a data example and apply equation \ref{eq:mlp} in each node to retrieve our binary result. This prediction step is called \textit{inference}. But in order to retrive meaningful results, we have to train our network first.

\subsection{Network Training}

The final goal of training such a network is to end up with a model that generalizes on any kind of data from the same class \parencite[p. 2]{pattern_and_ml}. The data that is used during this process is called \textit{training set}, the data that evaluates its generalization capabilites \textit{test set}. Since we know the ground truth outcome of this data example during the training phase, we can quantify the outcome using a loss function\footnote{Also known as cost function, objective function or error function. We use the averaged variants for all presented functions, because therefore we achieve pixel-wise results that are independent regarding the image dimensions later on when performing frame prediction. Additionally, the non-averaged versions of MAE and MSE are known as $ \ell_{1} $ and $ \ell_{2} $.}, such as \textit{mean absolute error} (MAE):

% TODO: always use loss functions in context of images? Wiki: PSNR defines the MSE pixel-wise

\begin{equation} \label{eq:mae}
  \mathcal{L}_{\textrm{mae}}(w, b)=\frac{1}{N} \sum\limits_{x} | y(x) - t(x) | ,
\end{equation}

\textit{mean squared error} (MSE):

\begin{equation} \label{eq:mse}
  \mathcal{L}_{\textrm{mse}}(w, b)=\frac{1}{N} \sum\limits_{x} \| y(x) - t(x) \|^2 ,
\end{equation}

or \textit{binary cross-entropy} (BCE) \parencite{conv_lstm_nowcasting}:

\begin{equation} \label{eq:bce}
  \mathcal{L}_{\textrm{bce}}(w, b)= -\frac{1}{N} \sum\limits_{x} t(x) \log{(y(x))} + (1-t(x)) \log{(1-y(x))} ,
\end{equation}

where $ N $ is the number of examples and $ t(x) $ denotes the ground truth target of an input example $x$. Many other functions exist, but these are the main objectives that we use in later chapters. During training our network, we want to find the set of weights $ w $ and biases $ b $ that minimizes our error:

\begin{equation} \label{eq:min-loss}
  \textrm{arg}\min_{w, b} \mathcal{L}(w, b) .
\end{equation}

Parameters beside $ w $ and $ b $ that are not learned during this process are called \textit{hyperparameters}. An example of such an non-trainable parameters are the number of layers or the size of each single layer. More hyperparameters will arise in the next sections.


\subsubsection{Neurons and Activations}

At this point, we face the fundamental problem of perceptrons. In order find the best set of parameters, we have to do small changes in $ w $ and $ b $ to justify the output into the right direction of the desired outcome. But since the perceptron's output is discrete, a small change can cause a sudden flip in the overall output. To overcome this issue, we replace these perceptrons with \textit{neurons}:

\begin{equation}
\begin{aligned}
z &= \sum\limits_{i=1}^n \, w_{i}x_{i} + b \\
y &= \phi(z) ,
\end{aligned}
\end{equation}

which allow $ x_{i}, y \in \mathbb{R} $ by wrapping its term with a non-linear \textit{activation function} $ \phi(z) $. Frequently used examples are the sigmoid function $ \sigma(z) $, hyperbolic tangent $ tanh(z) $ and the rectified linear unit (ReLu) $ max(0, z) $ (see Figure \ref{fig:activations}). The examplary structre of a neuron is illustrated in Figure \ref{fig:neuron}.

\begin{figure}[htpb]
	\centering
	\includegraphics[scale=0.6]{figures/neuron.pdf}
	\caption[Schematic neuron]{Schematic structure of a neuron with its $ n $ inputs $x_{i}$, weights $w_{i}$, bias $ b $ and activation function $\phi(z)$.} \label{fig:neuron}
\end{figure}

Note that the sigmoid function's shape is a smoothed out variant of the \textit{step function} \parencite{neural_nets_deep_learning}, which can be used to make a neuron act like a perceptron. Additionally, the rectifier differs to both other activation functions in that it is one-sided and partly linear. Even its shape looks much simpler, it became the most favorable activation function for intermediate layers in deep neural networks, as it allows faster computation, sparse activation\footnote{{A sparse activation means that only half of the neurons have an initial non-zero output, when a uniform initialization is used.}}, reduces the likelihood of vanising gradient (see \ref{sec:rnn-drawbacks}) and is more biologically plausible \parencite{relu}.

\begin{figure}[htpb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        ymin=-1,
        ymax=2,
        xmin=-3,
        xmax=3,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=a(z),
        xlabel=z
      ]
      \addplot [mark=none,draw=blue,smooth,ultra  thick] {1/(1+exp(-1*(\x))};
      \addlegendentry{sigmoid};
      \addplot [mark=none,draw=red,smooth,ultra thick] {tanh(\x)};
      \addlegendentry{tanh};
      \addplot[mark=none,draw=black!30!green,ultra thick,smooth,domain=0:3] {x};
	  \addplot[mark=none,draw=black!30!green,ultra thick,smooth,domain=-3:0] {0};
      \addlegendentry{ReLu};
    \end{axis}
  \end{tikzpicture}
  \caption[Activation functions]{Diagram showing the most common activation functions in neural networks.}\label{fig:activations}
\end{figure}

\subsubsection{Initialization}

Before starting the training process, we have to assign each variables $ w $ and $ b $ an initial value. This is done by pure randomness, using for example a uniform or gaussian distribution. But if we start with weights that are too small, the signal can decrease so much that it is to small to be usefull. On the other side, when we initialize our parameters with high values, the signal can end up to explode while propagating through the network \parencite{understand_xavier}. In consequence, a good initialization has a radical effect on how fast the network will learn useful patterns.

For this purpose, some best practices have been developed. One famous example that is used throughout in our final model is \textit{Xavier initialization} (eq. \ref{eq:xavier}). Its formulation is based on the number of input and output neurons and uses sampling from an uniform distribution with zero mean and all biases set to zero \parencite{xavier-init}:

\begin{equation} \label{eq:xavier}
  W \sim \textrm{U} \bigg[-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\bigg] ,
\end{equation}

where $ W $ is the weight matrix at each layer, $ n_{in} $ the number of incoming connections and $ n_{out} $ the number of outgoing connectons to the next layer. This initialization is designed to keep the gradients in all layers within approcimately the same scale.

\subsubsection{Backpropagation Learning Algorithm}

To acually train the network by minimizing its error (eq. \ref{eq:min-loss}), we apply a learning alogrithm called \textit{backpropagation}. This algorithm is based on \textit{gradient descent}, which iteratively tries to find the minima of a function by doing small steps towards the negative gradient. Applying this to our loss function results in the \textit{update rule} for all weights and biases:

\begin{equation} \label{eq:gradient_descent}
\begin{aligned}
w_{i}^{(\tau + 1)} &= w_{i}^{(\tau)} - \eta \frac{\partial \mathcal{L}}{\partial w_{i}^{(\tau)}} \\
b_{j}^{(\tau + 1)} &= b_{j}^{(\tau)} - \eta \frac{\partial \mathcal{L}}{\partial b_{j}^{(\tau)}} ,
\end{aligned}
\end{equation}

where $ \eta > 0 $ is the \textit{learning rate} that determines the size of step we do along the slope in each iteration \parencite{pattern_and_ml}. In other words, in every training iteration, we proceed backwards through our network and slightly adjust every parameter depending on how much it has contributed to the error. Doing a single step by computing the gradients for the whole training set would require to much time and memory resources. Hence, we estimate the gradients over the whole population by using a smaller sample. This technique is called \textit{stochastic gradient descent} (SGD), whereas the size of the sample is known as \textit{batch size}.

Although this algorithm is really powerful, it comes with some disadvantages that have to be kept in mind. First, the result can converge to any local minimum. In consequence, finding a global minimum is not guaranteed. Secondly, depending on the choice of the learning rate $ \eta $, the algorithm might converge very slowly or even not at all \parencite{ann}.

Beside SGD, many other advanced gradient descent based optimization algorithms exist. Detailed explainations and visualizations can be found in \parencite{optimization}. The optimizer that is used in this thesis is called \textit{adaptive moments estimation} (Adam). This algorithm is based on adaptive estimates of lower-order moments and performs a form of step size annealing by using exponential moving averages of the parameters. Therefore, it usually requries less tuning of the learning rate and has shown to work well in practice \parencite{adam}.


\subsubsection{Stopping Criteria}

The training process can run endless. Therefore, a rule should be defined when to stop it. There are many options when to cancel the training. Also, combinations of different stopping criterias are possible. These can be for example:

\begin{itemize}
\item When the validation loss does not decrease (for a specified number of iterations).
\item When the change in loss falls below a defined threshold (for a specified number of iterations).
\item When a fixed number of steps or epochs\footnote{A single epoch is usually defined as the number of steps that is required to iterate over the whole trainng set.} elapses.
\item When a defined timeframe exceeds.
\end{itemize}


\subsection{Regularization}

As already stated, our goal is to find a representation that generalizes well. One common problem that has to be prevented when a neural network is trained is the effect of \textit{overfitting}. This means that even the training loss decreases further and further, the validation and test error suddenly starts to get worse. One cause might be that the size of the training set is not large enough. But to come up with more data is often not possible. Another reason might be that our \textit{model complexity}, so the total number of trainable parameters to high. To get an idea about the reason for this, imagine we want to fit a function $g(x)$ using some noisy data points of a ground truth function $f(x)$. When our model exhibits to many parameters, it might come up with a function that fits perfectly to all data points. Nevertheless, as demonstrated in figure \ref{fig:overfitting}, this is a bad regression of the unterlying function $f(x)$.

\begin{figure}[htpb]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        ymin=-2,
        ymax=6,
        xmin=0,
        xmax=8,
        legend style={legend pos=south east},
        grid,
        thick,
        ylabel=y,
        xlabel=x,
        scatter/classes={%
		a={mark=triangle*,black!30!green}}
      ]
      \addplot [mark=none,draw=blue,smooth,ultra thick, domain=0:8] {
		 0.21212121212121271*x^0
   		+2.6607142857142843*x^1
  		-0.64502164502164450*x^2
   		+0.049242424242424192*x^3  
      };
      \addlegendentry{ground truth f(x)};
      \addplot[scatter,only marks,%
		scatter src=explicit symbolic]%
	table[meta=label] {
	x     y      label
	0     0      a 
	1     3      a
	2     2.5    a 
	3     4      a 
	4     4      a 
	5     3      a 
	6     4      a 
	7     4      a 
	};
	\addlegendentry{measured data points};
	\addplot [mark=none,draw=red,smooth,ultra thick, domain=0:8] {
		-0.0000000003717474*x^0
   		+14.921428855475133*x^1
  		-22.184722849828937*x^2
   		+13.906250509793455*x^3
  		-4.2638890895127091*x^4
   		+0.67083337448114122*x^5
  		-0.051388893117505295*x^6
   		+0.0014880954099440117*x^7
      };
      \addlegendentry{overfitted g(x)};
    \end{axis}
  \end{tikzpicture}
  \caption[Regularization and overfitting]{Visualization of an overfitted function.}\label{fig:overfitting}
\end{figure}

On the other hand, a reduction of model complexity can also be a false conclusion because this limits the potential power of the network as well. Fortunately, research has originated different methods to master this issue.

A well known technique to delimitate overfitting is to penetalize high parameter values which cause the oscillation effect that can be seen in figure \ref{fig:overfitting}. Therefore, we extend our loss function with an additional regularization term. This method is called weight decay:

\begin{equation} \label{eq:reg-loss}
  \mathcal{L}_{total}(w, b)= \mathcal{L}(w, b) + \frac{\lambda}{N} \sum\limits_{w}w^2 ,
\end{equation}

where the coefficient $ \lambda $ controls the influence of the regularization. The example shown in eq. \ref{eq:reg-loss} uses an $ \ell_{2} $ regularizer over all weigths, which strongly penetalizes a high magnitude of values. Together with the learning rate $ \eta $, both define two of the most important hyperparameters in any neural network model. Finding appropriate values is a major task when finetuning the model.

A second regularization approach is known as \textit{dropout}. Instead of modifying the cost function, it manimulates a model's layer where it is applied on by randomly deactivating a neuron with a probability $p$ in every training step. As a result, the networks learns a robustness against distint patterns that cause a high activation towards a certain output. Stated differently, the network is forced not to learn any shortcut that could damage generality. It is to add that no neuron is deactivated during inference. But to compensate the higher amout of active neurons within the layer, all weights of outgoing connections will be multiplied by $ p $ \parencite[p. 1931]{dropout}



\section{Convolutional Neural Networks}

In the previous section, we have discovered neural networks that exhibits a full connection of neuros from one layer to the next. While this allows to learn complex representions on the one hand, it comes with a couple of downside on the other hand as well. For example, data such as images would require the nework's layers to become large. Consequently, the number of connections between these layers would increase exponentially and thus the amound of trainable parameters as well. At the bottom line, this could end up in a network that is either time-consuming to train, or we are even not able to store it in memory. In addition, we would not not take any advantage of local image properties into account.

Therefore, a new network type found attention in recent years that are known as \textit{convolutional neural networks} (CNN). It is inspired by the animals visual cortex, has already been used in the late nineties to solve optical character recognition tasks (OCR) \parencite{lecun_conv}, but received its main attention after beating proven methods in the ImageNet competition by a large margin \parencite{imagenet}. The structure of a convolutional network, the detailed benefits and its mathematical formulation is described in the following sections.


\subsection{Structure}

A network is called CNN, when it consists of at least one convolutional layer. In other words, ``\textit{convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.}'' \parencite{deep_learning}. The definition of the convolution operation follows in section \ref{sec:conv-op}. Simply put, imagine a small window that slides across the input data. In every iteration, it attempts to extract features that are only dependent on a small neighbouring region with the size of this window. Moreover, the location of features that it tries to detect is not fixed to any specific spot, as it treats every patch in the same way. In every convolutional layer, this process is repeated several times, resulting in multiple feature maps. Figure \ref{fig:cnn-structure} visualizes the described structure of a simplified convolutional neural network.

\begin{figure}[htpb]
	\centering
	\includegraphics[scale=0.45]{figures/cnn_structure.png}
	\caption[Structure of a CNN]{Example of a simplified CNN structure with two convolutional layers for image classification.} \label{fig:cnn-structure}
\end{figure}

The window mentioned before is called \textit{kernel} and holds the randomly initialized parameters that the network can learn. The kernel acts as a filter that is applied to each location in a regular steps. In the two dimensional case, the kernal has a specified width and height, denoted as \textit{kernel size}. Several kernals are used to extract multiple feature maps in each convolutional layer, but each output feature map is computed with its own kernel. This number of kernels is specified with its \textit{kernel depth}. Furthermore, the step range the filter is moved in each dimension is called \textit{stride} \parencite{conv_guide}.

Each convolutioal layers is usually followed by a non-liner acitivation function, preferably a rectifier. The reason is that the convolution is an affine transformation and it therefore linear. Stacking multiple linear operations could be mathematically reduced to as single one. Optionally, an additional \textit{pooling layer} can be applied that performs a subsampling onto the feature maps. Several pooling variants exist, while \textit{max pooling} is probably the most frequently used of them. It allows the representation to become roughly invariant to small rotations or translations of the input \parencite[p. 343]{deep_learning} by only using the maximum value.


\subsection{Convolution Operation} \label{sec:conv-op}

Generally speaking, the convolution in an mathematical operation on two functions $f(x)$ and $g(x)$. Its operator is typically denoted with an asterisk \parencite[p. 332]{deep_learning} and is defined as:

\begin{equation} \label{eq:conv-general}
  (f \ast g)(x) = \int f(\tau)g(x-\tau) d\tau .
\end{equation}

In terminology of convolutional networks, the function $f$ is termed as the \textit{input} and the filter $g$ is referred to as the \textit{kernel}. Moreover, the output of $ (f \ast g)(x) $ is called the \textit{feature map}.

As we are dealing with discrete 2D-images in this thesis, the formulation of equation \ref{eq:conv-general} can be discritized and reformulated as:

\begin{equation} \label{eq:conv-2d}
  (I \ast K)(x,y) = \sum\limits_{r=0}^{h-1} \sum\limits_{c=0}^{w-1} I(c,r)K(x-c,y-r) ,
\end{equation}

with an input $ I $ of size $w \times h$ and a two-dimensional kernel $ K $. Depending on the size of the kernel with $ k \times k $ and the chosen stride $ s $, the shape of the convolved output changes. This is why in input is often enriched with a \textit{zero-padding} to have more control regarding the resulting output size. The use of no padding ($p=0$) is also called \textit{valid padding} (Figure \ref{fig:conv_valid}). Also, when a padding of $p=\floor{k/2}$ is used with a kernal size, it is referred to as \textit{same padding} because the input and output size stay unchanged in case of $ s=1 $ (Figure \ref{fig:conv_same}). 

\begin{figure}[htpb]
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{figures/conv_valid.png}
  \caption{$p=0$ (valid), $s=1$}
  \label{fig:conv_valid}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/conv_same.png}
  \caption{$p=1$ (same), $s=2$}
  \label{fig:conv_same}
\end{subfigure}
\caption[Convolution operation]{Visualizations of the convolutional operation with an $3 \times 3$ kernel but different settings for padding and stride.}
\label{fig:conv}
\end{figure}

It must be noted that the size of the kernel, padding and stride does not have to be equal in each dimension. But nevertheless, this is the case in most practical applications.

\subsection{Transposed Convolution Operation}

The application of the previously presented convolutional operation usually transforms the input into lower-dimensional feature maps. However, there are use cases where we would like to go the other way round, while keeping the connectivity pattern of a convolution. One example is an convolutional autoencoder which is explained in further detail in section \ref{sec:autoencoder}. This operation is referred to as \textit{transposed convolution}\footnote{Mistakenly, the transposed convolution is often called \textit{deconvolution}. But because it is not actually performing the reverse effect of a convolution, which is meant by the mathematical term of a deconvolution, it is strongly discouraged to name it so.}, which exchanges the forward and backward passes of a normal convolution. It is also called \textit{fractionally strided convolution}, because it can be emulated with a direct convolution using a zero-spaced input \cite[p. 19]{conv_guide}. Such an implementation is less efficient, but it supports the intuition of how the resulting output shape looks like. Figure \ref{fig:conv_tp} shows an example of a transposed convolution.

\begin{figure}[htpb]
	\centering
	\includegraphics[scale=0.4]{figures/conv_tp.png}
	\caption[Transposed convolution operation]{Transposition of convolving an $6 \times 6$ input using a $3 \times 3$ kernel using $p=1$ and $s=2$. This is equivalent to performing an convolution using zero-space $3 \times 3$ input with $p=1$ and $s=1$.} \label{fig:conv_tp}
\end{figure}


\subsection{Advantages}

To sum up the benefits of an convolutional network, the three central design ideas are \textit{sparse connections}, \textit{parameter sharing} and \textit{equivariance to translation} \parencite[p. 336ff.]{deep_learning}.

\subsubsection*{Sparse Connections}
The kernel used in a convolution is smaller than in input. Consequently, we have to store fewer parameters, as well as can take advantage of local relationships present in the data. This also leads to a higher training efficiency and a radical reduction of memory requirements.

\subsubsection*{Parameter Sharing}
To handle all regions of the input data in the same manner, the parameters are reused at every location as well. This is implemented by making use of only a single kernel which holds all learnable parameters.  Additionally, this decreases the number of parameters even further. To that end, figure \ref{fig:conv_vs_fc} compares the connection pattern  and the sharing of model parameters of fully-connected layers against the convolutional case.

\begin{figure}[htpb]
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/fc2.pdf}
  \caption{}
  \label{fig:conv_vs_fc_fc}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/param_share.pdf}
  \caption{}
  \label{fig:conv_vs_fc_conv}
\end{subfigure}
\caption[Sparse connections and parameter sharing in a CNN]{Comparison of the connection pattern and usage of parameters between (a) fully-connected layers and (b) convolutional layers. The use of the same color for interconnections denote the sharing of parameters.}
\label{fig:conv_vs_fc}
\end{figure}

\subsubsection*{Equivariance to Translation}
The sharing of parameters leads to the third advantage. Because the kernel and its paramters are reused at every position, the model learns the same representations at every position \parencite[p. 339]{deep_learning}. For example, if an input image wis translated by a fixed number of pixels, the network would handle it in the same way.

Unfortunately, convolutions are not tolerant to other transformations from the ground up. But to counteract this, other techniques exist such as a subsequent pooling stage to enable slight rotation invariance, as already introduced before.


\subsection{Fully-Convolutional Networks}

The complete use of convolutional layers implicates a fourth advantage over fully-connected layers. Since the kernel size in each layer is independent regarding its input, the overall network could be feed with data of different dimension. In contrary, this is not possible anymore as soon as a single FC-layer is used at inference because its fixed-sized weight matrix has to be applied to the entire input, not just a local region. Nevertheless, this does not imply that no fully-connected layer can be used when training the network. Depending on the architecture, FC-layers can be used in components of the network that are only used during training. This advantage is for example taken into account when training a \textit{deep convolutional generative adversarial network} (DCGAN)\footnote{Novel network training strategy for generative networks, where a generator network G competes agains a second discriminator network D in an alternating fashion. Further details in \parencite{gan}.}, whose discriminator network is only used while training.

Regarding the problem of frame prediction we want to solve within this thesis, we have to deal with a huge amount of data in every training iteration. Therefore, we take advantage from this \textit{fully-convolutional network} (FCN) approach and design our architecture in such a way that we are able train neural network model on small image patches only. Afterwards, we can theoratically perform frame prediction on the whole image given a sequence of frames.


\section{Recurrent Neural Networks}

All previously presented network architectures suffer from one missing characteristic. Their memory is kind of static and predictions are mostly based on the current inputs only. Consequently, they are hard to be applied on problems where data reveals some sequential or temporal properties. Two examples are handwriting recognition, were the understanding of previous words is required to deduce the current word's context. Also, as in our case the knowledge of the past image frames to be able to predict the future frames that naturally matches to the given previous sequence. A topology that addresses this issue are \textit{recurrent neural networks} (RNN). In this section, we will give an overview about their structure and formal description, as well as present a succession model that addresses its fundamental problems. It is to add that the whole section is mainly inspired by the great explainations in \parencite{understand_lstm}.


\subsection{Basics}

RNNs are a special class of neural networks that allows its models to form a directed cyclic graph. Thereby, they are able to hold an hidden state that represents the sequential dynamics. Given an input sequence $ X = (x^{(1)}, x^{(2)},..., x^{(\tau)}) $, the $ \tau^{th} $ recurrent building block gets $x^{(\tau)}$ as its input of this sequence, as well as the hidden state $h^{(\tau-1)}$ of the previous cell. These building blocks are typically referred to as a \textit{cell}. Because the first cell has has no predecessor, its hidden state input $ h^{(0)} $ is usually manually fed with an zero-initialized state vector. Formally, we can describe an RNN as follows:

\begin{equation} \label{eq:rnn}
  h^{(\tau)} = \phi(W_{h} h^{(\tau-1)} + W_{x} x^{(\tau)} + b) ,
\end{equation}

where $ W_{h} \in \mathbb{R}^{d_h \times d_h} $ are the weights of the hidden to hidden transitions, $ W_{x} \in \mathbb{R}^{d_x \times d_h} $ the weights of the input to hidden transitions and $ b, h^{(0)} \in \mathbb{R}^{d_h} $ the bias weights and initial state, respectively \parencite[p. 2]{rnn-batchnorm}. The activation function $ \phi(z) $ is usually chosen to be tanh.

Like convolutional networks, RNNs take advantage of sharing parameters over different parts of the model \parencite[p. 374]{deep_learning}. But in this case, we share model parameters over the temporal domain. This allows to generalize specific properties across the whole input sequence. Consequently, a model can extract patterns that can occur at any or even multiple positions within the sequence of data.

\subsubsection{Structure}

For a better understanding of how recurrent networks work, it is helpful to take a look on its graphical model that was formally described by equation \ref{eq:rnn}. As it can be seen in Figure \ref{fig:rnn-loop}, the hidden state transition can be compactly visualized using a loop. These loops represent the influence of the past values with respect to the current value. However, to have a representation that is analogous to the already shown model, it is possible to unroll the loop to convert it back to a DAG (see Figure \ref{fig:rnn-unrolled}).

\begin{figure}[htpb]
\centering
\begin{subfigure}{0.25\textwidth}
  \centering
  \includegraphics[scale=0.5]{figures/rnn_loop.pdf}
  \caption{}
  \label{fig:rnn-loop}
\end{subfigure}%
\begin{subfigure}{0.75\textwidth}
  \centering
  \includegraphics[scale=0.5]{figures/rnn_unrolled.pdf}
  \caption{}
  \label{fig:rnn-unrolled}
\end{subfigure}
\caption[Structure of RNN cells]{Structure of recurrent network cells. The compact cyclic graph model in (a) can be unrolled to receive the model (b) that represents the model in form of a acyclic graph.}
\label{fig:rnn}
\end{figure}

Moreover, the framework for recurrent models is very flexible as well. Depending on the implementation, it allows to process either a fixed or even a dynamic number of inputs. This extends to the number of outputs as well. In contrary, (convolutional) neural networks require to define the input and output size as design time. Further, they have to process all data in one chunk and do not allows to handle only single elements of the sequence one after the other. Some example modes of recurrent networks are visualized in Figure \ref{fig:rnn-modes}.

\begin{figure}[htpb]
\centering
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/one2many.pdf}
  \caption{}
  \label{fig:rnn-one2many}
\end{subfigure}%
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/many2one.pdf}
  \caption{}
  \label{fig:rnn-many2one}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/many2many.pdf}
  \caption{}
  \label{fig:rnn-many2many}
\end{subfigure}
\caption[Modes in recurrent networks]{Examples of different modes in recurrent networks: (a) one-to-many, (b) many-to-one, (c) many-to-many. Red squares denote the inputs, gray squares the recurrent cells and all outputs are colored in blue. (Based on \parencite{rnn-effectiveness})}
\label{fig:rnn-modes}
\end{figure}


\subsubsection{Backpropagation Trough Time}

Analyzing the RNN structure might raise the question of how this effects the training procedure, because the gradient flows through recurrent cells multiple times while backpropagating the error. It is important to see that recurrent networks are still feed-forward networks with the extension to keep reusing the same weights expanded in time. Consequently, we have to propagate the error backwards starting from the last timestep $ \tau $ like in standard backpropagation. Depending on the length of the sequence and the computational resources, we can proceed until the very beginning, or truncate our view of interest by stopping at a given limit. Additionally, given the shared weights $ w $ of two timesteps $\tau $ and $ \tau+1 $, we have to hold the weight constraint $ w^{(\tau)} = w^{(\tau+1)} $. Therefore, $ \nabla w^{(\tau)} = \nabla w^{(\tau+1)} $ has to be fulfilled as well. We can do so by by computing the gradients for both timesteps independently, but use the average of their sum:

\begin{equation} \label{eq:rnn}
	\frac{\partial \mathcal{L}}{\partial w^{(\tau)}} + \frac{\partial \mathcal{L}}{\partial w^{(\tau+1)}} ,
\end{equation}

when the final model parameter update is performed using the update rule \parencite{rnn-bptt}. This principle can be extened to the total sequence that is considered by the recurrent network and is referred to as \textit{backpropagation through time} (BPTT).

\subsubsection{Drawbacks} \label{sec:rnn-drawbacks}

Keeping the previously explained weight constraints in mind, the recurrent network has to perform a lot of correlated updates of the same model parametes at once. This is actually bad for stochastic gradient descent, as it prefers uncorrelated parameters for the stability of the training. Especially when a sequence gets quite long, this can yield mathematical instability due to many multiplications using the same shared weights. On the one side, the gradients can grow exponentially and become infinite \parencite{lstm}. On the other side, the network could learn nothing because the gradients vanish. This issue is known as the \textit{vanishing and exploding gradient problem}. The lack of learning long-term dependencies in recurrent networks has been identified in \parencite{hochreiter} and \parencite{rnn-vanish}. Fortunately, other RNN variants exist that can deal with long-term depencecies. The currently most prominent version is introduced in the following section.


\subsection{Long Short-Term Memory}

Initially introduced by Hochreiter and Schmidhuber, the \textit{long short-term memory} (LSTM) became kind of the default recurrent network implementation as it is capable to deal with long range dependencies. Over the years, it has been revised by a couple of follow-up studies \parencite{lstm_peep} \parencite{lstm_v2} and is used in many practical applications today.

The central advancement of LSTMs over traditional recurrent networks is the so called \textit{memory cell state} $C^{(\tau)}$. While a simple RNN cell overides its state at each timestep, the LSTM's memory cell updates exhibit only minor linear interaction, so that information could flow through very easily. Moreover, this simplifies the gradient flow backwards through time \parencite{rnn-batchnorm}. It follows that an an LSTM cell inverts the core issue it tries to solve. Instead of learning to remember things, it is actually trained to learn what can be forgotten. In this way, keeping information over a longer period of time became its default setting.

To regulate the update of the internal memory state, the LSTM introduces the use of an attentive gating mechanism. At each timestep, it is regulated by three trainable gates in order to accumulate or remove content from its state. Firstly, the \textit{input gate} determines the flow of information from the current input $ x^{(\tau)} $. Secondly, the \textit{forgot gate} regulates to which extend we keep information from the past cell state $C^{(\tau-1)}$. And lastly, the LSTM output state $h^{(\tau)}$ is specified by filtering the cell state using the \textit{output gate}. This mechanism enables constant error flow to build a long-term understanding even when applied to long sequences. It can be formalized as:

\begin{equation} \label{eq:lstm}
\begin{aligned}
\Spvek{\tilde{f}^{(\tau)}; \tilde{i}^{(\tau)}; \tilde{o}^{(\tau)}; \tilde{g}^{(\tau)}} &= W_{h} h^{(\tau-1)} + W_{x} x^{(\tau)} + b \\
C^{(\tau)} &= \sigma(\tilde{f}^{(\tau)}) \odot C^{(\tau-1)} + \sigma(\tilde{i}^{(\tau)}) \odot tanh(\tilde{g}^{(\tau)}) \\
h^{(\tau)} &= \sigma(\tilde{o}^{(\tau)}) \odot tanh(C^{(\tau)})
\end{aligned} ,
\end{equation}

where $ W_h \in \mathbb{R}^{d_h \times 4d_h} $ are the shared weights for the hidden to hidden transitions at timestep $ \tau $, $ W_x \in \mathbb{R}^{d_x \times 4d_h} $ the shared weigths for the input to hidden connections, $ b \in \mathbb{R}^{4d_h} $ the biases, and $ C^{(0)}, h^{(0)} \in \mathbb{R}^{4d_h} $ the initial states of the memory cell and the hidden state, respectively \parencite{rnn-batchnorm}. The last dimension of the weight matrices and biases are multiples of four, because the matrices are concatenations of weights for all three gates and the new candidate cell state for computational efficiency. The input, forget and output gates are labled as $ i, f $ and $ o $. Furthermore, the operator $ \odot $ denotes the Hadamard product\footnote{The Hadamard product defines the entrywise product of two matrices with the same dimension.} and a tilde indicates the term before the corresponding activation is applied, so that for example $ f^{(\tau)} = \sigma(\tilde{f}^{(\tau)}) $.


\subsubsection{Structure}
%Show differences to classical RNNs
% All gates use sigmoid: "0: let nothin through/delete, 1: let everything through"

% g_t: new candiate cell state, that could be added to the cell state (further regulated by the input-gate)

% LSTM = FC-LSTM (and why)

As depicted in Figure \ref{fig:lstm}, the internal structure of an LSTM cell looks way more complex compared to a simple RNN (see Figure \ref{fig:rnn-unrolled}). Nevertheless, the role of each single building block has actually a quite simple interpretation.

\begin{figure}[htpb]
	\centering
	\includegraphics[scale=0.8]{figures/lstm.pdf}
	\caption[Structure of a LSLTM cell]{Structure of a LSTM cell. The flow of the cell state is highlighted in bold. Gated unites are emphasized with a dotted box. (Based on \parencite{understand_lstm})} \label{fig:lstm}
\end{figure}

As stated before, the fundamental idea of LSTMs is the cell state $C^{(\tau)}$, denoted as a bold line in the graphics above. Frome its previous cell state to the next, there are just two interaction points where its content gets manipulated. First, using a pointwise multipliciation the the output of the forget gate $f^{(\tau)}$, a specific part of the previous state can be party or fully deleted. Afterwards, only regulated by in input gate $ i^{(\tau)} $ the new candidate cell state $ g^{(\tau)} $ is added to it. The third gate $ o^{(\tau)} $ has no effect on the cell state itself and governs the hidden output only.
The interpretation of how each single gated unit works is the following. Each gate itself is a fully-connected neural network layer, that receives the sum of the input $x^{(\tau)}$ and the previous hidden state $h^{(\tau-1)}$ as its input. That is the reason why they are also often referred to as \textit{FC-LSTM}. Additioanlly, as it can be seen in Figure \ref{fig:lstm}, every gate uses sigmoid as its activation function that is followed by an Hadamard product. Consequently, the output of any gate is in range $ [0, 1] \in \mathbb{R}^{d_h \times 4d_h} $, where a value of one means let the whole information flow through, while zero intends to forget everything. 

\subsubsection{Variants}

The long short-term memory belongs to the family of \textit{gated RNNs} \parencite[p. 411]{deep_learning}. Since its invention, a couple of related implementations have been introduced. One example extends the LSTM by having so called \textit{peephole connections}, proposed in \parencite{lstm_peep}. These additional connections have to purpose that each gating unit has an access to the previous memory cell state $ C^{(\tau-1)} $. The motivation for this is to allow the units to learn when to open or close their gates in order to learn more precise timings based on the cell state \parencite{lstm-space}.

Another example is the \textit{gated recurrent unit} (GRU) \parencite{gru}, which also regulates the flow of information comparable to an LSTM, but without having a seperate memory cell. Additionally, its gates for input and output are merged to a single \textit{update gate}, which leads to a similar performance but with a lower memory footprint \parencite{gru-video}. The forgot gate in context of a GRU is called \textit{reset gate}.

A further, novel variant of LSTMs will be presented in chapter \ref{chapter:implementation}, which is inspired by the strengths of convolutional neural networks. Aside from that, it will formalize the implementation of peephole connections as well.



\section{Autoencoder Networks} \label{sec:autoencoder}

%What are autoencoder networks?
%Or: Encoder / Decoder Networks


\section{Batch Normalization}

Training a deep neural network model is said to be very hard. One reason for this is that every layer not just has to learn the overall representation directly from the beginning, but also has to master the change in distribution of its inputs, given all preceding layers. As an example, consider one layer in the middle of a deep network. At training time, its adjustments to the model parameters depend on its inputs, which is equivalent to the output of the previous layer. But the fact that the previous layer learns and modifies its weights and biases as well follows that the intput to the following layer can change over time significantly, especially at the very beginning of the training due to random initialization. This ongoing change in the feature's distribution during training is known as \textit{internal covariance shift} \parencite{rnn-batchnorm}.

One modern practice that tries to overcome this issue by minimizing the covariate shift is called \textit{batch normalization} \parencite{batchnorm}. It performans a normalizaton on the inputs to a layer and transforms it to have a mean of zero and a standard deviation of one. Its mathematical formulation is:

\begin{equation} \label{eq:bn}
\textrm{BN}(\textbf{x}; \gamma, \beta) = \beta + \gamma \frac{\textbf{x} - \mathbb{E}(\textbf{x})}{\sqrt{Var(\textbf{x}) + \epsilon}} ,
\end{equation}

where $ \textbf{x} \in \mathbb{R}^d $ is the output of the previous layer to be normalized, $\gamma \in \mathbb{R}^d $ and $\beta \in \mathbb{R}^d $ the shift and scale of the distribution that are learned, as well as $\epsilon \in \mathbb{R} $ that serves as a regularization parameter and to avoid dividing by zero \parencite{rnn-batchnorm}.

When batch normalization is applied, we have to consider two different modes during training and inference. On the one hand at training time, batch normalization takes advantage of two simplifications, because full whitening of inputs is very expensive. First, each feature dimension is normalized independently. Secondly, the statistics of $ \mathbb{E}(\textbf{x}) $ and $ Var(\textbf{x}) $ are estimated using the sample mean and sample variance based on each mini-batch of stochastic gradient training. On the other hand during inference, the statistics have to be assessed using the entire training data. One way to do so is to track to track the moving averaged mean and variance parameters of each batch-estimate while training the model, and finally use these averages when a prediction is performed. That is the way how batch normalization is used throughout this work.

The advantages of batch normalization are versatile:

\begin{itemize}
\item Compensation against internal covariate shift to achieve a stable distribution of activation values.
\item Separate normalization of individual feature dimensions, so that features are not decorrelated.
\item Speed-up training, because it allows to use higher learning rates.
\item Practice has shown that even a higher accuracy can be achieved compared to non-batch-normalized versions of a network model \parencite[p. 7]{batchnorm}.
\end{itemize}


\section{Image Similarity Assessment}

The practical use of deep learning in context of image generation tasks received a tremendous increse in recent years. Examples include denoising autoencoders (see section \ref{sec:autoencoder}) for image reconstruction, semantic image completion \parencite{sem-img-inpainting}, the determination of optical flow \parencite{flownet}, \parencite{flow-static-img} or future  frame prediction of a given image sequence. Last mentioned field of application in is scope of this thesis. 

A neural networks main driver in respect to learning a good respresentation is the loss layer. Unfortunately, it found only little attention in most existing studies when applied in image processing tasks, at least until this year \parencite{loss-func-img-proc}. Therefore, we provide an overview of the main challenges of image similarity assessment and introduce some approved methods, as well as novel approaches when training neural networks in context of pictures in this section.


\subsection{Naive Approach}

The de facto standard loss function to compare a generated image with its ground truth target has been the \textit{pixel-wise squared error} (or $ \ell_{2} $) that was already mentioned in equation \ref{eq:mse}. It is easy to apply and usually provided by any neural network framework out-of-the-box. But on the other end, it suffers from some well-known limitations such as poor correlation with the human sense of perceptual quality \parencite{loss-func-img-proc}. The reason for this are twofold. First, $ \ell_{2} $ makes the assumption of a Gaussian noise model, which has its downsides when being applied to multimodal distributions. Second, it strongly penalizes outliers independently from local characteristics of the image, such as contrast or luminance. In contrary, the visual system of humans perceives noise in a different way.

\begin{figure}[htpb]
\centering
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/chess1.pdf}
  \caption{}
  \label{fig:percept-vs-lp1}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/chess2.pdf}
  \caption{}
  \label{fig:percept-vs-lp1}
\end{subfigure}
\caption[Example: Weakness of $ \ell_{2} $]{Example to demonstrate the weakness of $ \ell_{2} $ in context of perceptual image similarity.} \label{fig:chessfield}
\end{figure}

As an extreme but simple example, consider two images containing a chess field of black and white pixels. From an aerial perspective, both images look completely identical, because it is perceived as an a gray surface. Additionally, even when looking on it from a closer dinstance, we would still evaluate them as quite similar for the very reason that both images provide the same colors, structure, sharpness or contrast. Unfortunately, assessing both images using the given squared error function results in the maximum possible difference. The reason for this is that in considers the squared differences of related pixels only. Even that this specific weakness is not solved by using the $ \ell_{1} $ loss, several studies have shown that the usage of the absolute error function slightly reduces the blur effect on edges in image generation processes of natural images \parencite{loss-func-img-proc} \parencite{deep_multiscale_video_pred}. Some example image reconstructions comparing different loss functions are show in Figure \ref{fig:percept-vs-lp}.

\begin{figure}[htpb]
\centering
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/img-loss-comp1.png}
  \caption{}
  \label{fig:percept-vs-lp1}
\end{subfigure}%
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/img-loss-comp1.png}
  \caption{}
  \label{fig:percept-vs-lp1}
\end{subfigure}
\caption[Comparison of Reconstructions with Differnt Loss Functions]{Comparison of Reconstructions with Differnt Loss Functions using images from the STL-10 dataset: (a) where results using a perceptual motivated loss function was ranked first by human judgement, (b) where MAE or MSE was ranked best. (From \parencite{learning-perc-sim})}
\label{fig:percept-vs-lp}
\end{figure}


\subsection{Perceptual Quality Metrics}
% both, some are simple, others are perceptuall motivated ==> will use them in evaluation

The example of the previous section has shown that the consideration of our human visual perception is very important when assessing image similarity. With this in mind, a couple of metrics have been developed in recent years to evaluate image quality differences between two images. These metrics can be used either for a quatitative evaluation of generated results, or even as a loss function by doing some minor modifications to fulfill the required properties. Beside neural network training, these metrics have been invented to measure the quality of image compression codecs such as JPEG.

In the following sections, $ \textbf{x} $ will denote the ground truth image and $ \textbf{y} $ its generated reconstruction.

\subsubsection*{Peak Signal-to-Noise Ratio}

A first metric to assess the the similarity of generated images is the \textit{peak signal-to-noise ratoin} (PSNR). It describes the ratio between the maximum possible image intensity and the corrupting noise that affects precision of the reconstruction. Its value is expressed in a logarithmic decibel scale, where a higher value indicates better quality. In terms of human perception, it is an rough approximation to evaluate reconstruction quality because its denominator is still based on MSE. It is computed as follows:

\begin{equation} \label{eq:psnr}
\textrm{PSNR}(\textbf{x}, \textbf{y}) = 10 \log_{10} \frac{\textbf{y}_{max}^2}{\frac{1}{w \, h} \sum_{c=1}^{w} \sum_{r=1}^{h} (\textbf{x}_{c,r} - \textbf{y}_{c,r})^2 } ,
\end{equation}

where $ \textbf{y}_{max} $ is the maximum \textit{possible} intensity of an image of size $ w \times h $.


\subsubsection*{Structual Similarity}

For predicting the perceived image quality, the \textit{structural similarity} (SSIM) index invented in \parencite{ssim} can be used as a second assessment criteria. This full reference metric\footnote{Full reference metric (FR) is a quality term that means that the evaluation is based on every pixel of the entire ground truth image as a reference.} is an improvement over PSNR as it is based on assumptions of the human vision system. Therefore, it assesses both images based on luminance $ l(\textbf{x}, \textbf{y}) $ , contrast $ c(\textbf{x}, \textbf{y}) $ and structural similarity $ s(\textbf{x}, \textbf{y}) $ \parencite{ms-ssim}. It as defines as follows:

\begin{equation} \label{eq:ssim-components}
\begin{aligned}
l(\textbf{x}, \textbf{y}) &= \frac{2 \mu_x \mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1} \\
c(\textbf{x}, \textbf{y}) &= \frac{2 \sigma_x \sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2} \\
s(\textbf{x}, \textbf{y}) &= \frac{2 \sigma_{xy} + C_3}{\sigma_x \sigma_y + C_3} ,
\end{aligned}
\end{equation}

where $ \mu_x $, $ \sigma_x $ and $ \sigma_{xy} $ is the mean, standard deviation and covariance of $ x $ and $ y $, respectively. Further, $ C_1 = (K_1 L)^2 $, $ C_2 = (K_2 L)^2 $ and $ C_3 = \frac{C_2}{2} $ are small constants for numerical stability, $K_1 = 0.01$ and $ K_2 = 0.03 $ by default and $ L=255 $ the typical dynamic range of pixel-values for 8-bit \textit{gray-scale} images. These terms can be combined to define the SSIM index given by:

\begin{equation} \label{eq:ssim-combined}
\textrm{SSIM}(\textbf{x}, \textbf{y}) = \big[ l(\textbf{x}, \textbf{y})\big]^{\alpha} \cdot \big[c(\textbf{x}, \textbf{y}) \big]^{\beta} \cdot \big[ s(\textbf{x}, \textbf{y}) \big]^{\gamma} ,
\end{equation}

where $ \alpha $, $ \beta $ and $ \gamma $ parameterize the relative relative importance of all three components, typically $ \alpha = \beta = \gamma = 1 $. The terms for contrast and structure can be further simplified to $ cs(\textbf{x}, \textbf{y}) $ \parencite[p. 5]{loss-func-img-proc}, resulting in:

\begin{equation} \label{eq:ssim-simplified}
\begin{aligned}
\textrm{SSIM}(\textbf{x}, \textbf{y}) &= \frac{2 \mu_x \mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1} \cdot \frac{2 \sigma_{xy} + C_2}{\sigma_x^2 + \sigma_y^2 + C_2} \\
&= l(\textbf{x}, \textbf{y}) \cdot cs(\textbf{x}, \textbf{y}) .
\end{aligned}
\end{equation}

The SSIM index can be computed using a sliding window approach \parencite{ssim-slide}. Therefore, a square kernel of size $ 11 \times 11 $\footnote{The initial paper states to use an $ 8 \times 8 $ window. But many open source libraries, and even the MATLAB implementation of the paper's author itself use a kernel size of $ 11 \times 11 $.} and \textit{valid} padding is used that moves over the whole image, pixel by pixel. The index is then calculated in every local region and finally averaged to recieve the overall image quality for evaluation. The metric value is in range $ SSIM(\textbf{x}, \textbf{y}) \in [0, 1] $, where a higher value indicates more similarity.

\subsubsection*{Multi-Scale Structural Similarity}

Further studies have shown that the viewing conditions can have a tremendous influence on the perceived image similarity. Therefore, there SSIM index has been extended to incorporate the image on $ M $ different scale, where $ M=1 $ indicates the full-size image that is iteratively downsampled by a factor of two. This metric is known as the \textit{multi-scale structural similarity} (MS-SSIM) index for images \parencite{ms-ssim}. It is given by:

\begin{equation} \label{eq:ms-ssim}
\textrm{MS-SSIM}(\textbf{x}, \textbf{y}) = \big[ l_M(\textbf{x}, \textbf{y})\big]^{\alpha_M} \cdot \prod\limits_{j=1}^{M} \big[c_j(\textbf{x}, \textbf{y}) \big]^{\beta_j} \cdot \big[ s_j(\textbf{x}, \textbf{y}) \big]^{\gamma_j} ,
\end{equation}

where the exponents $ \alpha_M $, $ \beta_j $ and $ \gamma_j $ parameterize the relative importance of each component. The luminance $ l_M(\textbf{x}, \textbf{y}) $ difference is only computed for the smallest image size at scale $ M $. As a simple standard selection for the exponents, we can use $ \alpha_M = \beta_j = \gamma_j $ and $ \sum_{j=1}^{M} \gamma_j = 1 $. But by performing an empirical study in \parencite{ms-ssim}, the authors propose to use $ \beta_1 = \gamma_1 = 0.0448 $, $ \beta_2 = \gamma_2 = 0.2856 $, $ \beta_3 = \gamma_3 = 0.3001 $, $ \beta_4 = \gamma_4 = 0.2363 $ and $ \alpha_5 = \beta_5 = \gamma_5 = 0.1333 $ by incorporating $ M=5 $ scales. As a downside, evaluating multiple scales can be computational expensive. Additionally, the selected window size has to be smaller than the image at scale $ M $, and has to have an appropriate minimum image size due to the algorithms iterative downsampling.


\subsubsection*{Sharpness Difference}

To quantatatively evaluate the difference in sharpness between two images, we take use of the \textit{sharpness difference} metric proposed in \parencite{deep_multiscale_video_pred}. It is based on the formulation of PSNR (eq. \ref{eq:psnr}) with a modified denominator. Instead of using the squared error to quantify the pixel-wise differences, it measures the difference of gradients between the ground truth and its reconstruction:

\begin{equation} \label{eq:sharpdiff}
\textrm{SharpDiff}(\textbf{x}, \textbf{y}) = 10 \log_{10} \frac{\textbf{y}_{max}^2}{\frac{1}{w \, h} \sum_{c=2}^{w} \sum_{r=2}^{h} \big|(\nabla_{left} \, \textbf{x} + \nabla_{top} \, \textbf{x})-(\nabla_{left} \, \textbf{y} + \nabla_{top} \, \textbf{y})\big|} ,
\end{equation}

where $ \nabla_{left} \, \textbf{x} = |\textbf{x}_{c,r} - \textbf{x}_{c-1, r}| $ and $ \nabla_{top} \, \textbf{x} = |\textbf{x}_{c,r} - \textbf{x}_{c, r-1}| $ are the gradient differences to the left and top pixel.

\subsection{Perceptual Motivated Loss Functions}

Due to the lacking consideration of human perceptional qualities like sharpness, constrast or structure of standard error functions, new forms of losses have to be considered when a neural network is trained to solve image processing tasks.

\subsubsection*{Structural Loss}

The differentiability of the SSIM index makes it a well suited candiate to be used in neural network training. However, due to the fact that $ \textrm{SSIM}(\textbf{x}, \textbf{x}) = 1 $, it does not fulfill all required properties  of a loss function. Fortunately, this can be rectified by exchanging the minimum and maximum value of the metric:

\begin{equation} \label{eq:sharpdiff}
\mathcal{L}_{\textrm{ssim}}(\textbf{x}, \textbf{y}) = 1 - \textrm{SSIM}(\textbf{x}, \textbf{y}).
\end{equation}

Of course, the same priciple can be applied to end up with the multi-scale structural loss function:

\begin{equation} \label{eq:sharpdiff}
\mathcal{L}_{\textrm{ms-ssim}}(\textbf{x}, \textbf{y}) = 1 - \textrm{MS-SSIM}(\textbf{x}, \textbf{y}).
\end{equation}

Both error functions have been evaluated in context of image generation super-resolution and JPEG artifact removal in \parencite{learning-perc-sim} and \parencite{loss-func-img-proc}. The latter suggests to combine each of them with $ \ell_1 $ to get the best of both worlds.

\subsubsection*{Gradient Difference Loss}

Using the same criteria of the sharpness difference metric (see eq. \ref{eq:sharpdiff}), a strategy to further sharpen the image is to penalize the gradient differences in image space. This loss function is referred to as \textit{gradient difference loss} (GDL) and was proposed in \parencite{deep_multiscale_video_pred}. Combined with another error function, it can serve as an additional bias to deliver sharper results. To be more specific, the authores suggest to combine it with an $ \ell_1 $ loss function. The per-pixel GDL function to assess the ground truth with its corresponding reconstruction is defined as follows:

\begin{equation} \label{eq:sharpdiff}
\mathcal{L}_{\textrm{gdl}}(\textbf{x}, \textbf{y}) = \frac{1}{w \, h} \sum_{c=2}^{w} \sum_{r=2}^{h} \Big(\big|\nabla_{left} \, \textbf{y} - \nabla_{left} \, \textbf{x}\big|^{\alpha} + \big|\nabla_{top} \, \textbf{x} - \nabla_{top} \, \textbf{y}\big|^{\alpha}\Big) ,
\end{equation}

where $ \alpha \in \mathbb{N}^{+} $ is a parmeter to adjust the exponent. Typically, $ \alpha = 1 $ is chosen when combined with $ \ell_1 $ loss and $ \alpha = 2 $ in combination with $ \ell_2 $. With training efficiency in mind, the function describes the most simple image gradient by only considering the the intensitiy difference of the direct neighbor.